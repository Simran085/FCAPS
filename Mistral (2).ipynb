{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "461807a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0a66764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Version: 12.1\n",
      "GPU: NVIDIA H100 80GB HBM3 MIG 3g.40gb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b07f314-2933-4e5e-9e0f-ab53aef3af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b50cee30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LineId</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Pid</th>\n",
       "      <th>Level</th>\n",
       "      <th>Component</th>\n",
       "      <th>Content</th>\n",
       "      <th>EventId</th>\n",
       "      <th>EventTemplate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>81109</td>\n",
       "      <td>203615</td>\n",
       "      <td>148</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$PacketResponder</td>\n",
       "      <td>PacketResponder 1 for block blk_38865049064139...</td>\n",
       "      <td>E10</td>\n",
       "      <td>PacketResponder &lt;*&gt; for block blk_&lt;*&gt; terminating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>81109</td>\n",
       "      <td>203807</td>\n",
       "      <td>222</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$PacketResponder</td>\n",
       "      <td>PacketResponder 0 for block blk_-6952295868487...</td>\n",
       "      <td>E10</td>\n",
       "      <td>PacketResponder &lt;*&gt; for block blk_&lt;*&gt; terminating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>81109</td>\n",
       "      <td>204005</td>\n",
       "      <td>35</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.FSNamesystem</td>\n",
       "      <td>BLOCK* NameSystem.addStoredBlock: blockMap upd...</td>\n",
       "      <td>E6</td>\n",
       "      <td>BLOCK* NameSystem.addStoredBlock: blockMap upd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>81109</td>\n",
       "      <td>204015</td>\n",
       "      <td>308</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$PacketResponder</td>\n",
       "      <td>PacketResponder 2 for block blk_82291938032499...</td>\n",
       "      <td>E10</td>\n",
       "      <td>PacketResponder &lt;*&gt; for block blk_&lt;*&gt; terminating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>81109</td>\n",
       "      <td>204106</td>\n",
       "      <td>329</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$PacketResponder</td>\n",
       "      <td>PacketResponder 2 for block blk_-6670958622368...</td>\n",
       "      <td>E10</td>\n",
       "      <td>PacketResponder &lt;*&gt; for block blk_&lt;*&gt; terminating</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LineId   Date    Time  Pid Level                     Component  \\\n",
       "0       1  81109  203615  148  INFO  dfs.DataNode$PacketResponder   \n",
       "1       2  81109  203807  222  INFO  dfs.DataNode$PacketResponder   \n",
       "2       3  81109  204005   35  INFO              dfs.FSNamesystem   \n",
       "3       4  81109  204015  308  INFO  dfs.DataNode$PacketResponder   \n",
       "4       5  81109  204106  329  INFO  dfs.DataNode$PacketResponder   \n",
       "\n",
       "                                             Content EventId  \\\n",
       "0  PacketResponder 1 for block blk_38865049064139...     E10   \n",
       "1  PacketResponder 0 for block blk_-6952295868487...     E10   \n",
       "2  BLOCK* NameSystem.addStoredBlock: blockMap upd...      E6   \n",
       "3  PacketResponder 2 for block blk_82291938032499...     E10   \n",
       "4  PacketResponder 2 for block blk_-6670958622368...     E10   \n",
       "\n",
       "                                       EventTemplate  \n",
       "0  PacketResponder <*> for block blk_<*> terminating  \n",
       "1  PacketResponder <*> for block blk_<*> terminating  \n",
       "2  BLOCK* NameSystem.addStoredBlock: blockMap upd...  \n",
       "3  PacketResponder <*> for block blk_<*> terminating  \n",
       "4  PacketResponder <*> for block blk_<*> terminating  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hdfs_data = pd.read_csv(\"/workspace/FCAPS/dataset/HDFS_2k.log_structured.csv\")\n",
    "linux_data = pd.read_csv(\"/workspace/FCAPS/dataset/Linux_2k.log_structured.csv\")\n",
    "android_data = pd.read_csv(\"/workspace/FCAPS/dataset/Android_2k.log_structured.csv\")\n",
    "hadoop_data = pd.read_csv(\"/workspace/FCAPS/dataset/Hadoop_2k.log_structured.csv\")\n",
    "openStack_data = pd.read_csv(\"/workspace/FCAPS/dataset/OpenStack_2k.log_structured.csv\")\n",
    "\n",
    "hdfs_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5c04289",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LineId</th>\n",
       "      <th>Month</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Level</th>\n",
       "      <th>Component</th>\n",
       "      <th>PID</th>\n",
       "      <th>Content</th>\n",
       "      <th>EventId</th>\n",
       "      <th>EventTemplate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Jun</td>\n",
       "      <td>14</td>\n",
       "      <td>15:16:01</td>\n",
       "      <td>combo</td>\n",
       "      <td>sshd(pam_unix)</td>\n",
       "      <td>19939.0</td>\n",
       "      <td>authentication failure; logname= uid=0 euid=0 ...</td>\n",
       "      <td>E16</td>\n",
       "      <td>authentication failure; logname= uid=0 euid=0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jun</td>\n",
       "      <td>14</td>\n",
       "      <td>15:16:02</td>\n",
       "      <td>combo</td>\n",
       "      <td>sshd(pam_unix)</td>\n",
       "      <td>19937.0</td>\n",
       "      <td>check pass; user unknown</td>\n",
       "      <td>E27</td>\n",
       "      <td>check pass; user unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Jun</td>\n",
       "      <td>14</td>\n",
       "      <td>15:16:02</td>\n",
       "      <td>combo</td>\n",
       "      <td>sshd(pam_unix)</td>\n",
       "      <td>19937.0</td>\n",
       "      <td>authentication failure; logname= uid=0 euid=0 ...</td>\n",
       "      <td>E16</td>\n",
       "      <td>authentication failure; logname= uid=0 euid=0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Jun</td>\n",
       "      <td>15</td>\n",
       "      <td>02:04:59</td>\n",
       "      <td>combo</td>\n",
       "      <td>sshd(pam_unix)</td>\n",
       "      <td>20882.0</td>\n",
       "      <td>authentication failure; logname= uid=0 euid=0 ...</td>\n",
       "      <td>E18</td>\n",
       "      <td>authentication failure; logname= uid=0 euid=0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Jun</td>\n",
       "      <td>15</td>\n",
       "      <td>02:04:59</td>\n",
       "      <td>combo</td>\n",
       "      <td>sshd(pam_unix)</td>\n",
       "      <td>20884.0</td>\n",
       "      <td>authentication failure; logname= uid=0 euid=0 ...</td>\n",
       "      <td>E18</td>\n",
       "      <td>authentication failure; logname= uid=0 euid=0 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LineId Month  Date      Time  Level       Component      PID  \\\n",
       "0       1   Jun    14  15:16:01  combo  sshd(pam_unix)  19939.0   \n",
       "1       2   Jun    14  15:16:02  combo  sshd(pam_unix)  19937.0   \n",
       "2       3   Jun    14  15:16:02  combo  sshd(pam_unix)  19937.0   \n",
       "3       4   Jun    15  02:04:59  combo  sshd(pam_unix)  20882.0   \n",
       "4       5   Jun    15  02:04:59  combo  sshd(pam_unix)  20884.0   \n",
       "\n",
       "                                             Content EventId  \\\n",
       "0  authentication failure; logname= uid=0 euid=0 ...     E16   \n",
       "1                           check pass; user unknown     E27   \n",
       "2  authentication failure; logname= uid=0 euid=0 ...     E16   \n",
       "3  authentication failure; logname= uid=0 euid=0 ...     E18   \n",
       "4  authentication failure; logname= uid=0 euid=0 ...     E18   \n",
       "\n",
       "                                       EventTemplate  \n",
       "0  authentication failure; logname= uid=0 euid=0 ...  \n",
       "1                           check pass; user unknown  \n",
       "2  authentication failure; logname= uid=0 euid=0 ...  \n",
       "3  authentication failure; logname= uid=0 euid=0 ...  \n",
       "4  authentication failure; logname= uid=0 euid=0 ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linux_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f56caf8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LineId</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Pid</th>\n",
       "      <th>Tid</th>\n",
       "      <th>Level</th>\n",
       "      <th>Component</th>\n",
       "      <th>Content</th>\n",
       "      <th>EventId</th>\n",
       "      <th>EventTemplate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>03-17</td>\n",
       "      <td>16:13:38.811</td>\n",
       "      <td>1702</td>\n",
       "      <td>2395</td>\n",
       "      <td>D</td>\n",
       "      <td>WindowManager</td>\n",
       "      <td>printFreezingDisplayLogsopening app wtoken = A...</td>\n",
       "      <td>E100</td>\n",
       "      <td>printFreezingDisplayLogsopening app wtoken = A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>03-17</td>\n",
       "      <td>16:13:38.819</td>\n",
       "      <td>1702</td>\n",
       "      <td>8671</td>\n",
       "      <td>D</td>\n",
       "      <td>PowerManagerService</td>\n",
       "      <td>acquire lock=233570404, flags=0x1, tag=\"View L...</td>\n",
       "      <td>E10</td>\n",
       "      <td>acquire lock=&lt;*&gt;, flags=&lt;*&gt;, tag=\"&lt;*&gt;\", name=&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>03-17</td>\n",
       "      <td>16:13:38.820</td>\n",
       "      <td>1702</td>\n",
       "      <td>8671</td>\n",
       "      <td>D</td>\n",
       "      <td>PowerManagerService</td>\n",
       "      <td>ready=true,policy=3,wakefulness=1,wksummary=0x...</td>\n",
       "      <td>E103</td>\n",
       "      <td>ready=true,policy=&lt;*&gt;,wakefulness=&lt;*&gt;,wksummar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>03-17</td>\n",
       "      <td>16:13:38.839</td>\n",
       "      <td>1702</td>\n",
       "      <td>2113</td>\n",
       "      <td>V</td>\n",
       "      <td>WindowManager</td>\n",
       "      <td>Skipping AppWindowToken{df0798e token=Token{78...</td>\n",
       "      <td>E131</td>\n",
       "      <td>Skipping AppWindowToken{&lt;*&gt; token=Token{&lt;*&gt; Ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>03-17</td>\n",
       "      <td>16:13:38.859</td>\n",
       "      <td>2227</td>\n",
       "      <td>2227</td>\n",
       "      <td>D</td>\n",
       "      <td>TextView</td>\n",
       "      <td>visible is system.time.showampm</td>\n",
       "      <td>E165</td>\n",
       "      <td>visible is &lt;*&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LineId   Date          Time   Pid   Tid Level            Component  \\\n",
       "0       1  03-17  16:13:38.811  1702  2395     D        WindowManager   \n",
       "1       2  03-17  16:13:38.819  1702  8671     D  PowerManagerService   \n",
       "2       3  03-17  16:13:38.820  1702  8671     D  PowerManagerService   \n",
       "3       4  03-17  16:13:38.839  1702  2113     V        WindowManager   \n",
       "4       5  03-17  16:13:38.859  2227  2227     D             TextView   \n",
       "\n",
       "                                             Content EventId  \\\n",
       "0  printFreezingDisplayLogsopening app wtoken = A...    E100   \n",
       "1  acquire lock=233570404, flags=0x1, tag=\"View L...     E10   \n",
       "2  ready=true,policy=3,wakefulness=1,wksummary=0x...    E103   \n",
       "3  Skipping AppWindowToken{df0798e token=Token{78...    E131   \n",
       "4                    visible is system.time.showampm    E165   \n",
       "\n",
       "                                       EventTemplate  \n",
       "0  printFreezingDisplayLogsopening app wtoken = A...  \n",
       "1  acquire lock=<*>, flags=<*>, tag=\"<*>\", name=<...  \n",
       "2  ready=true,policy=<*>,wakefulness=<*>,wksummar...  \n",
       "3  Skipping AppWindowToken{<*> token=Token{<*> Ac...  \n",
       "4                                     visible is <*>  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "android_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b40bda4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LineId</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Level</th>\n",
       "      <th>Process</th>\n",
       "      <th>Component</th>\n",
       "      <th>Content</th>\n",
       "      <th>EventId</th>\n",
       "      <th>EventTemplate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-10-18</td>\n",
       "      <td>18:01:47,978</td>\n",
       "      <td>INFO</td>\n",
       "      <td>main</td>\n",
       "      <td>org.apache.hadoop.mapreduce.v2.app.MRAppMaster</td>\n",
       "      <td>Created MRAppMaster for application appattempt...</td>\n",
       "      <td>E29</td>\n",
       "      <td>Created MRAppMaster for application appattempt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-10-18</td>\n",
       "      <td>18:01:48,963</td>\n",
       "      <td>INFO</td>\n",
       "      <td>main</td>\n",
       "      <td>org.apache.hadoop.mapreduce.v2.app.MRAppMaster</td>\n",
       "      <td>Executing with tokens:</td>\n",
       "      <td>E42</td>\n",
       "      <td>Executing with tokens:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2015-10-18</td>\n",
       "      <td>18:01:48,963</td>\n",
       "      <td>INFO</td>\n",
       "      <td>main</td>\n",
       "      <td>org.apache.hadoop.mapreduce.v2.app.MRAppMaster</td>\n",
       "      <td>Kind: YARN_AM_RM_TOKEN, Service: , Ident: (app...</td>\n",
       "      <td>E61</td>\n",
       "      <td>Kind: YARN_AM_RM_TOKEN, Service: , Ident: (app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2015-10-18</td>\n",
       "      <td>18:01:49,228</td>\n",
       "      <td>INFO</td>\n",
       "      <td>main</td>\n",
       "      <td>org.apache.hadoop.mapreduce.v2.app.MRAppMaster</td>\n",
       "      <td>Using mapred newApiCommitter.</td>\n",
       "      <td>E111</td>\n",
       "      <td>Using mapred newApiCommitter.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2015-10-18</td>\n",
       "      <td>18:01:50,353</td>\n",
       "      <td>INFO</td>\n",
       "      <td>main</td>\n",
       "      <td>org.apache.hadoop.mapreduce.v2.app.MRAppMaster</td>\n",
       "      <td>OutputCommitter set in config null</td>\n",
       "      <td>E76</td>\n",
       "      <td>OutputCommitter set in config null</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LineId        Date          Time Level Process  \\\n",
       "0       1  2015-10-18  18:01:47,978  INFO    main   \n",
       "1       2  2015-10-18  18:01:48,963  INFO    main   \n",
       "2       3  2015-10-18  18:01:48,963  INFO    main   \n",
       "3       4  2015-10-18  18:01:49,228  INFO    main   \n",
       "4       5  2015-10-18  18:01:50,353  INFO    main   \n",
       "\n",
       "                                        Component  \\\n",
       "0  org.apache.hadoop.mapreduce.v2.app.MRAppMaster   \n",
       "1  org.apache.hadoop.mapreduce.v2.app.MRAppMaster   \n",
       "2  org.apache.hadoop.mapreduce.v2.app.MRAppMaster   \n",
       "3  org.apache.hadoop.mapreduce.v2.app.MRAppMaster   \n",
       "4  org.apache.hadoop.mapreduce.v2.app.MRAppMaster   \n",
       "\n",
       "                                             Content EventId  \\\n",
       "0  Created MRAppMaster for application appattempt...     E29   \n",
       "1                             Executing with tokens:     E42   \n",
       "2  Kind: YARN_AM_RM_TOKEN, Service: , Ident: (app...     E61   \n",
       "3                      Using mapred newApiCommitter.    E111   \n",
       "4                 OutputCommitter set in config null     E76   \n",
       "\n",
       "                                       EventTemplate  \n",
       "0  Created MRAppMaster for application appattempt...  \n",
       "1                             Executing with tokens:  \n",
       "2  Kind: YARN_AM_RM_TOKEN, Service: , Ident: (app...  \n",
       "3                      Using mapred newApiCommitter.  \n",
       "4                 OutputCommitter set in config null  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hadoop_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4732eca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LineId</th>\n",
       "      <th>Logrecord</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Pid</th>\n",
       "      <th>Level</th>\n",
       "      <th>Component</th>\n",
       "      <th>ADDR</th>\n",
       "      <th>Content</th>\n",
       "      <th>EventId</th>\n",
       "      <th>EventTemplate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>nova-api.log.1.2017-05-16_13:53:08</td>\n",
       "      <td>2017-05-16</td>\n",
       "      <td>00:00:00.008</td>\n",
       "      <td>25746</td>\n",
       "      <td>INFO</td>\n",
       "      <td>nova.osapi_compute.wsgi.server</td>\n",
       "      <td>req-38101a0b-2096-447d-96ea-a692162415ae 113d3...</td>\n",
       "      <td>10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4...</td>\n",
       "      <td>E25</td>\n",
       "      <td>&lt;*&gt; \"GET &lt;*&gt;\" status: &lt;*&gt; len: &lt;*&gt; time: &lt;*&gt;.&lt;*&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>nova-api.log.1.2017-05-16_13:53:08</td>\n",
       "      <td>2017-05-16</td>\n",
       "      <td>00:00:00.272</td>\n",
       "      <td>25746</td>\n",
       "      <td>INFO</td>\n",
       "      <td>nova.osapi_compute.wsgi.server</td>\n",
       "      <td>req-9bc36dd9-91c5-4314-898a-47625eb93b09 113d3...</td>\n",
       "      <td>10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4...</td>\n",
       "      <td>E25</td>\n",
       "      <td>&lt;*&gt; \"GET &lt;*&gt;\" status: &lt;*&gt; len: &lt;*&gt; time: &lt;*&gt;.&lt;*&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>nova-api.log.1.2017-05-16_13:53:08</td>\n",
       "      <td>2017-05-16</td>\n",
       "      <td>00:00:01.551</td>\n",
       "      <td>25746</td>\n",
       "      <td>INFO</td>\n",
       "      <td>nova.osapi_compute.wsgi.server</td>\n",
       "      <td>req-55db2d8d-cdb7-4b4b-993b-429be84c0c3e 113d3...</td>\n",
       "      <td>10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4...</td>\n",
       "      <td>E25</td>\n",
       "      <td>&lt;*&gt; \"GET &lt;*&gt;\" status: &lt;*&gt; len: &lt;*&gt; time: &lt;*&gt;.&lt;*&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>nova-api.log.1.2017-05-16_13:53:08</td>\n",
       "      <td>2017-05-16</td>\n",
       "      <td>00:00:01.813</td>\n",
       "      <td>25746</td>\n",
       "      <td>INFO</td>\n",
       "      <td>nova.osapi_compute.wsgi.server</td>\n",
       "      <td>req-2a3dc421-6604-42a7-9390-a18dc824d5d6 113d3...</td>\n",
       "      <td>10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4...</td>\n",
       "      <td>E25</td>\n",
       "      <td>&lt;*&gt; \"GET &lt;*&gt;\" status: &lt;*&gt; len: &lt;*&gt; time: &lt;*&gt;.&lt;*&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>nova-api.log.1.2017-05-16_13:53:08</td>\n",
       "      <td>2017-05-16</td>\n",
       "      <td>00:00:03.091</td>\n",
       "      <td>25746</td>\n",
       "      <td>INFO</td>\n",
       "      <td>nova.osapi_compute.wsgi.server</td>\n",
       "      <td>req-939eb332-c1c1-4e67-99b8-8695f8f1980a 113d3...</td>\n",
       "      <td>10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4...</td>\n",
       "      <td>E25</td>\n",
       "      <td>&lt;*&gt; \"GET &lt;*&gt;\" status: &lt;*&gt; len: &lt;*&gt; time: &lt;*&gt;.&lt;*&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LineId                           Logrecord        Date          Time  \\\n",
       "0       1  nova-api.log.1.2017-05-16_13:53:08  2017-05-16  00:00:00.008   \n",
       "1       2  nova-api.log.1.2017-05-16_13:53:08  2017-05-16  00:00:00.272   \n",
       "2       3  nova-api.log.1.2017-05-16_13:53:08  2017-05-16  00:00:01.551   \n",
       "3       4  nova-api.log.1.2017-05-16_13:53:08  2017-05-16  00:00:01.813   \n",
       "4       5  nova-api.log.1.2017-05-16_13:53:08  2017-05-16  00:00:03.091   \n",
       "\n",
       "     Pid Level                       Component  \\\n",
       "0  25746  INFO  nova.osapi_compute.wsgi.server   \n",
       "1  25746  INFO  nova.osapi_compute.wsgi.server   \n",
       "2  25746  INFO  nova.osapi_compute.wsgi.server   \n",
       "3  25746  INFO  nova.osapi_compute.wsgi.server   \n",
       "4  25746  INFO  nova.osapi_compute.wsgi.server   \n",
       "\n",
       "                                                ADDR  \\\n",
       "0  req-38101a0b-2096-447d-96ea-a692162415ae 113d3...   \n",
       "1  req-9bc36dd9-91c5-4314-898a-47625eb93b09 113d3...   \n",
       "2  req-55db2d8d-cdb7-4b4b-993b-429be84c0c3e 113d3...   \n",
       "3  req-2a3dc421-6604-42a7-9390-a18dc824d5d6 113d3...   \n",
       "4  req-939eb332-c1c1-4e67-99b8-8695f8f1980a 113d3...   \n",
       "\n",
       "                                             Content EventId  \\\n",
       "0  10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4...     E25   \n",
       "1  10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4...     E25   \n",
       "2  10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4...     E25   \n",
       "3  10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4...     E25   \n",
       "4  10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4...     E25   \n",
       "\n",
       "                                      EventTemplate  \n",
       "0  <*> \"GET <*>\" status: <*> len: <*> time: <*>.<*>  \n",
       "1  <*> \"GET <*>\" status: <*> len: <*> time: <*>.<*>  \n",
       "2  <*> \"GET <*>\" status: <*> len: <*> time: <*>.<*>  \n",
       "3  <*> \"GET <*>\" status: <*> len: <*> time: <*>.<*>  \n",
       "4  <*> \"GET <*>\" status: <*> len: <*> time: <*>.<*>  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openStack_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "843a4503",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in HDFS dataset:\n",
      "LineId           0\n",
      "Date             0\n",
      "Time             0\n",
      "Pid              0\n",
      "Level            0\n",
      "Component        0\n",
      "Content          0\n",
      "EventId          0\n",
      "EventTemplate    0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in Linux dataset:\n",
      "LineId             0\n",
      "Month              0\n",
      "Date               0\n",
      "Time               0\n",
      "Level              0\n",
      "Component          0\n",
      "PID              151\n",
      "Content            0\n",
      "EventId            0\n",
      "EventTemplate      0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in android dataset:\n",
      "LineId           0\n",
      "Date             0\n",
      "Time             0\n",
      "Pid              0\n",
      "Tid              0\n",
      "Level            0\n",
      "Component        0\n",
      "Content          0\n",
      "EventId          0\n",
      "EventTemplate    0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in hadoop dataset:\n",
      "LineId           0\n",
      "Date             0\n",
      "Time             0\n",
      "Level            0\n",
      "Process          0\n",
      "Component        0\n",
      "Content          0\n",
      "EventId          0\n",
      "EventTemplate    0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in openstack dataset:\n",
      "LineId           0\n",
      "Logrecord        0\n",
      "Date             0\n",
      "Time             0\n",
      "Pid              0\n",
      "Level            0\n",
      "Component        0\n",
      "ADDR             0\n",
      "Content          0\n",
      "EventId          0\n",
      "EventTemplate    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Missing values in HDFS dataset:\")\n",
    "print(hdfs_data.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in Linux dataset:\")\n",
    "print(linux_data.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in android dataset:\")\n",
    "print(android_data.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in hadoop dataset:\")\n",
    "print(hadoop_data.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in openstack dataset:\")\n",
    "print(openStack_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8b45b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_time_column(time_col):\n",
    "    time_col = time_col.astype(str).str.zfill(6)  \n",
    "    return time_col.str.slice(0, 2) + ':' + time_col.str.slice(2, 4) + ':' + time_col.str.slice(4, 6)\n",
    "\n",
    "hdfs_data_cleaned = hdfs_data.copy()\n",
    "hdfs_data_cleaned['FormattedTime'] = reformat_time_column(hdfs_data_cleaned['Time'])\n",
    "hdfs_data_cleaned['Timestamp'] = pd.to_datetime(\n",
    "    hdfs_data_cleaned['Date'].astype(str) + ' ' + hdfs_data_cleaned['Time'].astype(str), \n",
    "    format='%m%d%y %H%M%S',  \n",
    "    errors='coerce'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b59c5fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LineId</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Pid</th>\n",
       "      <th>Level</th>\n",
       "      <th>Component</th>\n",
       "      <th>Content</th>\n",
       "      <th>EventId</th>\n",
       "      <th>EventTemplate</th>\n",
       "      <th>FormattedTime</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>81109</td>\n",
       "      <td>203615</td>\n",
       "      <td>148</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$PacketResponder</td>\n",
       "      <td>PacketResponder 1 for block blk_38865049064139...</td>\n",
       "      <td>E10</td>\n",
       "      <td>PacketResponder &lt;*&gt; for block blk_&lt;*&gt; terminating</td>\n",
       "      <td>20:36:15</td>\n",
       "      <td>2009-08-11 20:36:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>81109</td>\n",
       "      <td>203807</td>\n",
       "      <td>222</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$PacketResponder</td>\n",
       "      <td>PacketResponder 0 for block blk_-6952295868487...</td>\n",
       "      <td>E10</td>\n",
       "      <td>PacketResponder &lt;*&gt; for block blk_&lt;*&gt; terminating</td>\n",
       "      <td>20:38:07</td>\n",
       "      <td>2009-08-11 20:38:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>81109</td>\n",
       "      <td>204005</td>\n",
       "      <td>35</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.FSNamesystem</td>\n",
       "      <td>BLOCK* NameSystem.addStoredBlock: blockMap upd...</td>\n",
       "      <td>E6</td>\n",
       "      <td>BLOCK* NameSystem.addStoredBlock: blockMap upd...</td>\n",
       "      <td>20:40:05</td>\n",
       "      <td>2009-08-11 20:40:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LineId   Date    Time  Pid Level                     Component  \\\n",
       "0       1  81109  203615  148  INFO  dfs.DataNode$PacketResponder   \n",
       "1       2  81109  203807  222  INFO  dfs.DataNode$PacketResponder   \n",
       "2       3  81109  204005   35  INFO              dfs.FSNamesystem   \n",
       "\n",
       "                                             Content EventId  \\\n",
       "0  PacketResponder 1 for block blk_38865049064139...     E10   \n",
       "1  PacketResponder 0 for block blk_-6952295868487...     E10   \n",
       "2  BLOCK* NameSystem.addStoredBlock: blockMap upd...      E6   \n",
       "\n",
       "                                       EventTemplate FormattedTime  \\\n",
       "0  PacketResponder <*> for block blk_<*> terminating      20:36:15   \n",
       "1  PacketResponder <*> for block blk_<*> terminating      20:38:07   \n",
       "2  BLOCK* NameSystem.addStoredBlock: blockMap upd...      20:40:05   \n",
       "\n",
       "            Timestamp  \n",
       "0 2009-08-11 20:36:15  \n",
       "1 2009-08-11 20:38:07  \n",
       "2 2009-08-11 20:40:05  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs_data_cleaned.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afcf45af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LineId Month  Date      Time  Level       Component      PID  \\\n",
      "0       1   Jun    14  15:16:01  combo  sshd(pam_unix)  19939.0   \n",
      "1       2   Jun    14  15:16:02  combo  sshd(pam_unix)  19937.0   \n",
      "2       3   Jun    14  15:16:02  combo  sshd(pam_unix)  19937.0   \n",
      "\n",
      "                                             Content EventId  \\\n",
      "0  authentication failure; logname= uid=0 euid=0 ...     E16   \n",
      "1                           check pass; user unknown     E27   \n",
      "2  authentication failure; logname= uid=0 euid=0 ...     E16   \n",
      "\n",
      "                                       EventTemplate           Timestamp  \n",
      "0  authentication failure; logname= uid=0 euid=0 ... 2023-06-14 15:16:01  \n",
      "1                           check pass; user unknown 2023-06-14 15:16:02  \n",
      "2  authentication failure; logname= uid=0 euid=0 ... 2023-06-14 15:16:02  \n"
     ]
    }
   ],
   "source": [
    "# Clean Linux dataset\n",
    "linux_data_cleaned = linux_data.dropna(subset=['PID']).copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "# Add a year (e.g., 2023) to the datetime string\n",
    "linux_data_cleaned['DateTime'] = '2023 ' + linux_data_cleaned['Month'] + ' ' + linux_data_cleaned['Date'].astype(str) + ' ' + linux_data_cleaned['Time']\n",
    "\n",
    "# Convert to Timestamp\n",
    "linux_data_cleaned['Timestamp'] = pd.to_datetime(\n",
    "    linux_data_cleaned['DateTime'],\n",
    "    format='%Y %b %d %H:%M:%S',  \n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "linux_data_cleaned = linux_data_cleaned.drop(columns=['DateTime'])\n",
    "print(linux_data_cleaned.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9531bec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LineId   Date      Time   Pid   Tid Level            Component  \\\n",
      "0       1  03-17  16:13:38  1702  2395     D        WindowManager   \n",
      "1       2  03-17  16:13:38  1702  8671     D  PowerManagerService   \n",
      "2       3  03-17  16:13:38  1702  8671     D  PowerManagerService   \n",
      "\n",
      "                                             Content EventId  \\\n",
      "0  printFreezingDisplayLogsopening app wtoken = A...    E100   \n",
      "1  acquire lock=233570404, flags=0x1, tag=\"View L...     E10   \n",
      "2  ready=true,policy=3,wakefulness=1,wksummary=0x...    E103   \n",
      "\n",
      "                                       EventTemplate           Timestamp  \n",
      "0  printFreezingDisplayLogsopening app wtoken = A... 1900-03-17 16:13:38  \n",
      "1  acquire lock=<*>, flags=<*>, tag=\"<*>\", name=<... 1900-03-17 16:13:38  \n",
      "2  ready=true,policy=<*>,wakefulness=<*>,wksummar... 1900-03-17 16:13:38  \n"
     ]
    }
   ],
   "source": [
    "# Clean Android dataset\n",
    "android_data_cleaned = android_data.copy()\n",
    "android_data_cleaned['Time'] = android_data_cleaned['Time'].str.split('.').str[0]  # Remove milliseconds\n",
    "android_data_cleaned['Timestamp'] = pd.to_datetime(\n",
    "    android_data_cleaned['Date'].astype(str) + ' ' + android_data_cleaned['Time'].astype(str),\n",
    "    format='%m-%d %H:%M:%S',  # Android dataset uses MM-DD HH:MM:SS format\n",
    "    errors='coerce'\n",
    ")\n",
    "print(android_data_cleaned.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5d729d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LineId        Date      Time Level Process  \\\n",
      "0       1  2015-10-18  18:01:47  INFO    main   \n",
      "1       2  2015-10-18  18:01:48  INFO    main   \n",
      "2       3  2015-10-18  18:01:48  INFO    main   \n",
      "\n",
      "                                        Component  \\\n",
      "0  org.apache.hadoop.mapreduce.v2.app.MRAppMaster   \n",
      "1  org.apache.hadoop.mapreduce.v2.app.MRAppMaster   \n",
      "2  org.apache.hadoop.mapreduce.v2.app.MRAppMaster   \n",
      "\n",
      "                                             Content EventId  \\\n",
      "0  Created MRAppMaster for application appattempt...     E29   \n",
      "1                             Executing with tokens:     E42   \n",
      "2  Kind: YARN_AM_RM_TOKEN, Service: , Ident: (app...     E61   \n",
      "\n",
      "                                       EventTemplate           Timestamp  \n",
      "0  Created MRAppMaster for application appattempt... 2015-10-18 18:01:47  \n",
      "1                             Executing with tokens: 2015-10-18 18:01:48  \n",
      "2  Kind: YARN_AM_RM_TOKEN, Service: , Ident: (app... 2015-10-18 18:01:48  \n"
     ]
    }
   ],
   "source": [
    "# Clean Hadoop dataset\n",
    "hadoop_data_cleaned = hadoop_data.copy()\n",
    "hadoop_data_cleaned['Time'] = hadoop_data_cleaned['Time'].str.split(',').str[0]  # Remove milliseconds\n",
    "hadoop_data_cleaned['Timestamp'] = pd.to_datetime(\n",
    "    hadoop_data_cleaned['Date'].astype(str) + ' ' + hadoop_data_cleaned['Time'].astype(str),\n",
    "    format='%Y-%m-%d %H:%M:%S',  # Hadoop dataset uses YYYY-MM-DD HH:MM:SS format\n",
    "    errors='coerce'\n",
    ")\n",
    "print(hadoop_data_cleaned.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ddb2398",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LineId                           Logrecord        Date      Time    Pid  \\\n",
      "0       1  nova-api.log.1.2017-05-16_13:53:08  2017-05-16  00:00:00  25746   \n",
      "1       2  nova-api.log.1.2017-05-16_13:53:08  2017-05-16  00:00:00  25746   \n",
      "2       3  nova-api.log.1.2017-05-16_13:53:08  2017-05-16  00:00:01  25746   \n",
      "\n",
      "  Level                       Component  \\\n",
      "0  INFO  nova.osapi_compute.wsgi.server   \n",
      "1  INFO  nova.osapi_compute.wsgi.server   \n",
      "2  INFO  nova.osapi_compute.wsgi.server   \n",
      "\n",
      "                                                ADDR  \\\n",
      "0  req-38101a0b-2096-447d-96ea-a692162415ae 113d3...   \n",
      "1  req-9bc36dd9-91c5-4314-898a-47625eb93b09 113d3...   \n",
      "2  req-55db2d8d-cdb7-4b4b-993b-429be84c0c3e 113d3...   \n",
      "\n",
      "                                             Content EventId  \\\n",
      "0  10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4...     E25   \n",
      "1  10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4...     E25   \n",
      "2  10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4...     E25   \n",
      "\n",
      "                                      EventTemplate           Timestamp  \n",
      "0  <*> \"GET <*>\" status: <*> len: <*> time: <*>.<*> 2017-05-16 00:00:00  \n",
      "1  <*> \"GET <*>\" status: <*> len: <*> time: <*>.<*> 2017-05-16 00:00:00  \n",
      "2  <*> \"GET <*>\" status: <*> len: <*> time: <*>.<*> 2017-05-16 00:00:01  \n"
     ]
    }
   ],
   "source": [
    "# Clean OpenStack dataset\n",
    "openstack_data_cleaned = openStack_data.copy()\n",
    "openstack_data_cleaned['Time'] = openstack_data_cleaned['Time'].str.split('.').str[0]  # Remove milliseconds\n",
    "openstack_data_cleaned['Timestamp'] = pd.to_datetime(\n",
    "    openstack_data_cleaned['Date'].astype(str) + ' ' + openstack_data_cleaned['Time'].astype(str),\n",
    "    format='%Y-%m-%d %H:%M:%S',  # OpenStack dataset uses YYYY-MM-DD HH:MM:SS format\n",
    "    errors='coerce'\n",
    ")\n",
    "print(openstack_data_cleaned.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68f59c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets sorted by Timestamp.\n"
     ]
    }
   ],
   "source": [
    "# Sort datasets by Timestamp\n",
    "hdfs_data_cleaned = hdfs_data_cleaned.sort_values(by=\"Timestamp\").reset_index(drop=True)\n",
    "linux_data_cleaned = linux_data_cleaned.sort_values(by=\"Timestamp\").reset_index(drop=True)\n",
    "android_data_cleaned = android_data_cleaned.sort_values(by=\"Timestamp\").reset_index(drop=True)\n",
    "hadoop_data_cleaned = hadoop_data_cleaned.sort_values(by=\"Timestamp\").reset_index(drop=True)\n",
    "openstack_data_cleaned = openstack_data_cleaned.sort_values(by=\"Timestamp\").reset_index(drop=True)\n",
    "\n",
    "print(\"Datasets sorted by Timestamp.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c3ffcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned HDFS dataset saved to: /workspace//FCAPS/dataset/Cleaned datasets/HDFS_2k_cleaned.csv\n",
      "Cleaned Linux dataset saved to: /workspace/FCAPS/dataset/Cleaned datasets/Linux_2k_cleaned.csv\n",
      "Cleaned Android dataset saved to: /workspace/FCAPS/dataset/Cleaned datasets/Android_2k_cleaned.csv\n",
      "Cleaned Hadoop dataset saved to: /workspace/FCAPS/dataset/Cleaned datasets/Hadoop_2k_cleaned.csv\n",
      "Cleaned OpenStack dataset saved to: /workspace/FCAPS/dataset/Cleaned datasets/Openstack_2k_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hdfs_cleaned_path = \"/workspace//FCAPS/dataset/Cleaned datasets/HDFS_2k_cleaned.csv\"\n",
    "linux_cleaned_path = \"/workspace/FCAPS/dataset/Cleaned datasets/Linux_2k_cleaned.csv\"\n",
    "android_cleaned_path= \"/workspace/FCAPS/dataset/Cleaned datasets/Android_2k_cleaned.csv\"\n",
    "hadoop_cleaned_path = \"/workspace/FCAPS/dataset/Cleaned datasets/Hadoop_2k_cleaned.csv\"\n",
    "openstack_cleaned_path = \"/workspace/FCAPS/dataset/Cleaned datasets/Openstack_2k_cleaned.csv\"\n",
    "\n",
    "hdfs_data_cleaned.to_csv(hdfs_cleaned_path, index=False)\n",
    "linux_data_cleaned.to_csv(linux_cleaned_path, index=False)\n",
    "android_data_cleaned.to_csv(android_cleaned_path, index=False)\n",
    "hadoop_data_cleaned.to_csv(hadoop_cleaned_path, index=False)\n",
    "openstack_data_cleaned.to_csv(openstack_cleaned_path, index=False)\n",
    "\n",
    "print(f\"Cleaned HDFS dataset saved to: {hdfs_cleaned_path}\")\n",
    "print(f\"Cleaned Linux dataset saved to: {linux_cleaned_path}\")\n",
    "print(f\"Cleaned Android dataset saved to: {android_cleaned_path}\")\n",
    "print(f\"Cleaned Hadoop dataset saved to: {hadoop_cleaned_path}\")\n",
    "print(f\"Cleaned OpenStack dataset saved to: {openstack_cleaned_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06aeab2c-ab02-4c50-838c-4b60184b2ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54de1eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading .env file from: /workspace/FCAPS/token.env\n",
      "Token successfully loaded!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "env_path = os.path.join(os.getcwd(), 'token.env')  # Current working directory + token.env\n",
    "print(f\"Loading .env file from: {env_path}\")  \n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN_mistral\")\n",
    "\n",
    "if hf_token:\n",
    "    print(\"Token successfully loaded!\")\n",
    "else:\n",
    "    print(\"Failed to load token. Check the .env file and its location.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab46a2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdcbb669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/FCAPS/prism_env/lib/python3.12/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59cdcac60f6498e9a96bed55e7436f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN_mistral\")\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=hf_token)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    token=hf_token,\n",
    "    use_cache=False,  \n",
    "    offload_buffers=True,\n",
    "    resume_download=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ab4f349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if \"model\" in locals():\n",
    "    print(next(model.parameters()).device)  \n",
    "else:\n",
    "    print(\"Model not loaded yet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07431eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG path added: /workspace/FCAPS/RAG\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca8d51666764fdf84836a71e527c384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/workspace/FCAPS/RAG/mistral_llm_loader.py:62: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=text_pipeline)\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "import sys\n",
    "import os\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "rag_path = \"/workspace/FCAPS/RAG\"\n",
    "if rag_path not in sys.path:\n",
    "    sys.path.append(rag_path)\n",
    "print(\"RAG path added:\", rag_path)\n",
    "\n",
    "# load llm_agent.py \n",
    "file_path = os.path.join(rag_path, \"llm_agent.py\")\n",
    "spec = importlib.util.spec_from_file_location(\"llm_agent\", file_path)\n",
    "llm_agent = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(llm_agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc0f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SIMPLE QUERIES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "784facdf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Show all SSH login failures in Linux logs (Intent: fcaps | FCAPS: Security | Risk: Medium)\n",
      "[Tool:fcaps] Logs sent to LLM:\n",
      " - authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root\n",
      "Intent → Fcaps | FCAPS → Security | Risk → Medium\n",
      "\n",
      "\n",
      "Based on the provided logs, it appears that there have been two failed SSH authentication attempts from the IP address 207.243.167.114 to the root user account on the system. This is a potential security threat as unauthorized access to the root account can lead to significant damage to the system.\n",
      "\n",
      "FCAPS Domain: Security\n",
      "Risk Level: Medium\n",
      "\n",
      "Mitigation:\n",
      "1. Change the root password immediately to prevent any unauthorized access.\n",
      "2. Implement multi-factor authentication for SSH access to the root account.\n",
      "3. Configure the firewall to block traffic from the IP address 207.243.167.114 unless it is known to be trusted.\n",
      "4. Review system logs regularly for any suspicious activity related to SSH authentication.\n",
      "5. Ensure that all software and security patches are up-to-date on the system.\n",
      "6. Consider implementing a centralized logging solution to collect and analyze SSH logs across multiple systems for better visibility and threat detection.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Show all SSH login failures in Linux logs\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51bdd380",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - List all ResourceManager heartbeats that failed in Hadoop logs. (Intent: semantic | FCAPS: Fault | Risk: Low)\n",
      "[Tool:semantic] Logs sent to LLM:\n",
      " - Diagnostics report from attempt_1445144423722_0020_m_000001_0: Error: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "- loaded properties from hadoop-metrics2.properties\n",
      "- Error writing History Event: org.apache.hadoop.mapreduce.jobhistory.TaskAttemptUnsuccessfulCompletionEvent@7317849d\n",
      "- Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\n",
      "- OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "Intent → Semantic | FCAPS → Fault | Risk → Low\n",
      "\n",
      "\n",
      "Based on the provided logs, it appears that there was an error in communication between a Node Manager (MININT-FNANLI5) and the JobTracker (msra-sa-41) during a MapReduce job execution attempt. The specific error message indicates a \"java.net.NoRouteToHostException\" which suggests a network connectivity issue. This error occurred when the Node Manager was attempting to contact the JobTracker on port 9000.\n",
      "\n",
      "The first log entry shows this error occurring during the diagnostics report for job attempt 1445144423722_0020_m_000001_0. The second log entry indicates that some properties were loaded from hadoop-metrics2.properties file. The third log entry signifies an unsuccessful completion event for a task attempt. The fourth log entry shows the addition of a global filter'safety'. Lastly, the fifth log entry specifies the OutputCommitter as org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.\n",
      "\n",
      "In order to diagnose and address the root cause of this issue, you should perform the following steps:\n",
      "\n",
      "1. Check the network connectivity between the Node Manager and the JobTracker. Ensure that both nodes can reach each other over the specified port (9000). You may need to check firewall rules, routing tables, or other network configurations.\n",
      "\n",
      "2. Verify that the JobTracker is running and accepting connections on port 9000. Use tools like netstat or jps to check the status of the JobTracker process.\n",
      "\n",
      "3. Inspect the YARN resource manager logs for any related errors or warnings. Look for entries around the time of the failed job attempt.\n",
      "\n",
      "4. Review the Hadoop configuration files, including core-site.xml, hdfs-site.xml, yarn-site.xml, and mapred-site.xml, to ensure proper settings for network and communication.\n",
      "\n",
      "5. Restart the affected services (Node Manager and JobTracker) to see if the issue resolves itself.\n",
      "\n",
      "6. If the problem persists, consider checking the Hadoop cluster's hardware resources, such as memory, CPU, or disk space, to ensure they meet the minimum requirements for optimal performance.\n",
      "\n",
      "By addressing these potential causes, you should be able to resolve the network connectivity issue and prevent future occurrences of the \"java.net.NoRouteToHostException\" error.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"List all ResourceManager heartbeats that failed in Hadoop logs.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25edf1a4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Identify PacketResponder termination events in HDFS logs. (Intent: semantic | FCAPS: Security | Risk: Low)\n",
      "[Tool:semantic] Logs sent to LLM:\n",
      " - Event Writer setup for JobId: job_1445144423722_0020, File: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445144423722_0020/job_1445144423722_0020_1.jhist\n",
      "- Default file system [hdfs://msra-sa-41:9000]\n",
      "- DFSOutputStream ResponseProcessor exception  for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743512_2731\n",
      "- Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 192 seconds.  Will retry shortly ...\n",
      "- Error writing History Event: org.apache.hadoop.mapreduce.jobhistory.TaskAttemptUnsuccessfulCompletionEvent@7317849d\n",
      "Intent → Semantic | FCAPS → Security | Risk → Low\n",
      "\n",
      "\n",
      "Based on the provided logs, it appears that there have been some issues related to HDFS (Hadoop Distributed File System) during a MapReduce job execution identified by JobId: job_144423722_0020. Specifically, there were errors related to an Event Writer setup, DFSOutputStream response processor exceptions, and failed leases for a DFSClient.\n",
      "\n",
      "However, none of these logs directly indicate any PacketResponder termination events. Instead, they suggest various problems encountered while processing data within the HDFS ecosystem during this MapReduce job execution.\n",
      "\n",
      "To further investigate potential PacketResponder termination events, you may want to check other relevant logs such as those from YARN (Yet Another Resource Negotiator), which manages the allocation of resources for MapReduce jobs. Look for logs containing messages like \"Container [<container_id>] exited with status [<exit_status>]\" or \"Application <application_id> finished with status <status>\" in the YARN logs. These logs might provide more information about the overall status of the MapReduce job and any container failures that could be linked to PacketResponder terminations.\n",
      "\n",
      "Additionally, you can examine the MapReduce application logs stored at `hdfs://msra-sa-41:9000/user/<username>/<application_name>/<application_attempt_id>/logs` for more detailed information about the job's progress and any errors encountered during task execution. This may help identify specific tasks where PacketResponder terminations occurred, if applicable.\n",
      "\n",
      "In summary, based on the given logs, no direct evidence of PacketResponder termination events was found. However, investigating additional logs from YARN and the MapReduce application itself may provide more insight into the issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Identify PacketResponder termination events in HDFS logs.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5265ccc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - List API requests to /servers/detail in OpenStack nova logs. (Intent: semantic | FCAPS: Configuration | Risk: Low)\n",
      "[Tool:semantic] Logs sent to LLM:\n",
      " - 10.11.21.139,10.11.10.1 \"GET /openstack/2013-10-17/vendor_data.json HTTP/1.1\" status: 200 len: 124 time: 0.2188199\n",
      "- 10.11.21.127,10.11.10.1 \"GET /openstack/2012-08-10/meta_data.json HTTP/1.1\" status: 200 len: 264 time: 0.2254272\n",
      "- 10.11.21.129,10.11.10.1 \"GET /openstack/2013-10-17/vendor_data.json HTTP/1.1\" status: 200 len: 124 time: 0.2354860\n",
      "- 10.11.21.133,10.11.10.1 \"GET /openstack/2013-10-17/user_data HTTP/1.1\" status: 404 len: 176 time: 0.2186041\n",
      "Intent → Semantic | FCAPS → Configuration | Risk → Low\n",
      "\n",
      "\n",
      "Based on the provided logs, there are no entries related to the specific endpoint \"/servers/detail\" in OpenStack Nova. However, we can observe some GET requests to the \"/openstack\" path with different sub-paths and timestamps. These requests seem to be fetching JSON data from the OpenStack server.\n",
      "\n",
      "To answer the user query, we need to look for logs containing the exact string \"/servers/detail\". Since the logs provided do not contain this string, it's essential to check other logs or use more comprehensive search queries to find relevant information.\n",
      "\n",
      "If you have access to more logs or can provide additional context, please let me know, and I will help you analyze them further. In the meantime, I recommend checking other logs that might contain the desired information, such as those generated by the OpenStack Horizon dashboard or the Nova compute nodes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"List API requests to /servers/detail in OpenStack nova logs.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b6c3c88",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Retrieve broadcast receivers triggered after BOOT_COMPLETED. (Intent: time | FCAPS: Security | Risk: Low)\n",
      "[Tool:time] Logs sent to LLM:\n",
      " - ready=true,policy=3,wakefulness=1,wksummary=0x0,uasummary=0x1,bootcompleted=true,boostinprogress=false,waitmodeenable=false,mode=false,manual=38,auto=-1,adj=0.0userId=0\n",
      "- ready=true,policy=3,wakefulness=1,wksummary=0x0,uasummary=0x1,bootcompleted=true,boostinprogress=false,waitmodeenable=false,mode=false,manual=38,auto=-1,adj=0.0userId=0\n",
      "Intent → Time | FCAPS → Security | Risk → Low\n",
      "\n",
      "\n",
      "Based on the provided logs, there are no explicit broadcast receivers mentioned in the data. However, we can identify that both logs indicate that the system has completed the boot process (`bootcompleted=true`). Therefore, to answer your query, we would need access to more detailed logs that include information about broadcast receivers and their triggering events.\n",
      "\n",
      "However, I can suggest an approach to monitor for potential issues related to BOOT_COMPLETED events and broadcast receivers based on the available logs:\n",
      "\n",
      "1. Analyze the frequency of BOOT_COMPLETED events: Check how often the system is completing the boot process. A high number of frequent boots could indicate instability or other issues.\n",
      "2. Monitor the duration between consecutive BOOT_COMPLETED events: If the time between consecutive boots is significantly shorter than expected, it could indicate a problem with the system's stability or resource utilization.\n",
      "3. Look for anomalous behavior in manual/auto mode transitions: The `manual` and `auto` fields in the logs indicate the current mode of operation. Anomalous transitions between these modes could be indicative of issues with the system's power management or user interaction.\n",
      "4. Investigate any adjustments to wakefulness or adj settings: Changes to these settings could impact the system's power consumption and overall performance. Keep track of any significant changes and investigate their root cause.\n",
      "5. Review the system's policy settings: Policy settings can affect the system's behavior, including its response to BOOT_COMPLETED events and broadcast receivers. Regularly review these settings to ensure they are optimized for the system's requirements.\n",
      "\n",
      "To implement this monitoring strategy, you can use a combination of log analysis tools and automated alerting systems. By regularly analyzing the logs and identifying any deviations from normal behavior, you can proactively address potential issues before they become critical.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Retrieve broadcast receivers triggered after BOOT_COMPLETED.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0788f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODERATE QUERIES: SLIGHTLY VAGUE OR MULTI-INTENT QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03e80b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Find unknown rhost login attempts and correlate with any sudo failures. (Intent: semantic | FCAPS: Security | Risk: Medium)\n",
      "[Tool:semantic] Logs sent to LLM:\n",
      " - authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root\n",
      "Intent → Semantic | FCAPS → Security | Risk → Medium\n",
      "\n",
      "\n",
      "Based on the provided logs, we have two instances of authentication failure for the root user via SSH with unknown IP addresses (207.243.167.114 and 150.183.249.110). These logs do not contain any information about successful logins or sudo usage. However, we can still infer some potential security concerns based on this data.\n",
      "\n",
      "Firstly, it's important to note that these logs indicate failed authentication attempts, meaning that the intruders were unable to gain access to the system using the provided IP addresses and root credentials. Nevertheless, repeated unsuccessful attempts could be an indicator of a brute force attack or an attempt to guess valid credentials.\n",
      "\n",
      "Secondly, since both failed attempts target the root user, they represent a significant threat as the root account has full administrative privileges. An attacker who gains access to this account can cause extensive damage to the system.\n",
      "\n",
      "To further investigate this issue, you may want to check the following:\n",
      "\n",
      "1. Review the system's firewall rules and access control lists (ACLs) to ensure that only authorized IP addresses are allowed to connect to the SSH service.\n",
      "2. Implement multi-factor authentication (MFA) for SSH logins to add an additional layer of security.\n",
      "3. Check the system logs for any other related events, such as failed sudo attempts or suspicious network activity.\n",
      "4. Monitor the system for any unusual patterns in failed authentication attempts, such as frequent attempts from specific IP addresses or unusual login times.\n",
      "5. Consider implementing intrusion detection systems (IDS) or Security Information and Event Management (SIEM) solutions to help identify and respond to potential threats more effectively.\n",
      "\n",
      "In conclusion, while the provided logs do not directly show any correlation between unknown rhost login attempts and sudo failures, they do indicate a potential security concern due to repeated failed attempts to authenticate as the root user from unknown IP addresses. Further investigation and remediation steps should be taken to mitigate the risk of unauthorized access to the system.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Find unknown rhost login attempts and correlate with any sudo failures.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9a22b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Analyze job submission failures possibly due to container saturation. (Intent: semantic | FCAPS: Fault | Risk: Low)\n",
      "[Tool:semantic] Logs sent to LLM:\n",
      " - User unknown timed out after 900 seconds at Sat Jun 18 02:23:10 2005\n",
      "- connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:03:04 2005\n",
      "- connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:02:55 2005\n",
      "Intent → Semantic | FCAPS → Fault | Risk → Low\n",
      "\n",
      "\n",
      "Based on the provided logs, it appears that there have been two failed connections originating from the same IP address (207.30.238.8, which resolves to host8.topspot.net). The first failure occurred for a user whose identity is unknown and who timed out after 900 seconds on June 18, 2005. The second and third entries represent successful connections from the same IP address on July 17, 2005.\n",
      "\n",
      "While these logs do not directly indicate container saturation, they do suggest potential network congestion or latency issues that could impact job submissions. It's important to note that the logs do not provide enough context to definitively diagnose the cause of the user timeout or the subsequent successful connections. However, based on best practices and general knowledge about cluster management, we can make some reasonable inferences and recommendations.\n",
      "\n",
      "First, let's consider the possibility of container saturation. Container saturation occurs when the number of running containers exceeds the available resources, leading to performance degradation and potential job submission failures. In this case, the logs don't provide any clear indication of container saturation since they don't mention any resource utilization metrics or error messages related to container limits.\n",
      "\n",
      "However, the logs do suggest potential network congestion or latency issues. Timeouts during job submission can be caused by various factors, including network connectivity problems, high load on the submission node, or insufficient resources allocated to the job. Since the logs show multiple failed connections from the same IP address, it's possible that there might be network congestion or latency issues affecting the client attempting to submit jobs.\n",
      "\n",
      "To further investigate and potentially mitigate the issue, you may want to consider the following steps:\n",
      "\n",
      "1. Monitor network traffic and identify any patterns or trends that could indicate network congestion or latency issues. This could involve analyzing network logs, using tools like Wireshark or tcpdump, or monitoring network utilization metrics.\n",
      "2. Check the system and application logs on the submission nodes and other relevant components to see if there are any recurring errors or warnings related to network connectivity, container allocation, or resource utilization.\n",
      "3. Review the configuration of your cluster and ensure that appropriate resources are allocated to handle expected workload. This could include increasing the number of worker nodes, adjusting container limits, or optimizing resource usage.\n",
      "4. Consider implementing measures to improve network performance, such as using dedicated network interfaces, optimizing routing configurations, or implementing traffic shaping policies.\n",
      "5. Communicate with the users experiencing submission failures to gather more information about their specific use cases and requirements. This could help inform decisions around resource allocation, network optimization, or other potential solutions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Analyze job submission failures possibly due to container saturation.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff9b49ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Show image usage verification failures during instance launches. (Intent: fcaps | FCAPS: Fault | Risk: Medium)\n",
      "[Tool:fcaps] Logs sent to LLM:\n",
      " - image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking\n",
      "- image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking\n",
      "Intent → Fcaps | FCAPS → Fault | Risk → Medium\n",
      "\n",
      "\n",
      "Based on the user query, it appears that the logs indicate image verification failures during instance launches in OpenStack Nova environment. This issue falls under the Fault domain of FCAPS as it relates to errors or exceptions occurring in the system. The risk level for this issue can be considered medium since image verification failures may lead to instances being launched with incorrect or outdated images, which could result in operational issues or security vulnerabilities.\n",
      "\n",
      "To mitigate the risk associated with image verification failures, consider implementing the following measures:\n",
      "\n",
      "1. Configure OpenStack to use trusted images from a verified repository. This can help minimize the risk of launching instances with incorrect or compromised images.\n",
      "\n",
      "2. Set up automatic image verification using tools like Glance or other third-party solutions. This will ensure that all images used for instance creation are properly verified before they are made available for use.\n",
      "\n",
      "3. Monitor the logs regularly for any image verification failure messages. If such messages are detected, investigate the root cause and take appropriate actions to resolve the issue promptly.\n",
      "\n",
      "4. Implement role-based access control policies to restrict access to image management functions to authorized personnel only. This can help prevent unintended changes to images and reduce the likelihood of image verification failures.\n",
      "\n",
      "5. Regularly update your OpenStack environment and its components to ensure that you have the latest security patches and bug fixes installed. This can help address known vulnerabilities and reduce the risk of image verification failures due to software bugs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Show image usage verification failures during instance launches.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39d24b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Detect unusual block verification errors from DataNodes. (Intent: semantic | FCAPS: Security | Risk: Low)\n",
      " No matching dataset found, falling back to all logs\n",
      " No matching dataset found, falling back to all logs\n",
      "[Tool:semantic] Logs sent to LLM:\n",
      " - Received block blk_6799391641508508235 of size 67108864 from /10.251.107.98\n",
      "- Received block blk_-7716943410942322104 of size 67108864 from /10.251.126.5\n",
      "Intent → Semantic | FCAPS → Security | Risk → Low\n",
      "\n",
      "\n",
      "Based on the provided logs, there are no explicit indications of unusual block verification errors from DataNodes. However, we can infer some information that might be relevant to detecting such errors in the future.\n",
      "\n",
      "Firstly, these logs show that two different DataNodes have transmitted blocks to the NameNode. The blocks' sizes are identical (67108864 bytes), which is not necessarily an issue but could be worth investigating further if other inconsistencies arise.\n",
      "\n",
      "Secondly, it's important to note that these logs represent successful block receptions by the NameNode. No error messages were recorded during these transactions. Therefore, the absence of error messages does not guarantee the absence of errors. Block verification errors may occur after these logs were generated, and they would typically result in HDFS reporting the corresponding data corruption or data loss.\n",
      "\n",
      "To effectively monitor for unusual block verification errors, you should consider implementing the following practices:\n",
      "\n",
      "1. Regularly check HDFS dumps for any signs of data corruption or data loss. This includes examining the EditsLog, FSImage, and other relevant files.\n",
      "2. Monitor HDFS metrics, such as the number of data corruption events, the amount of data lost due to corruption, and the overall health of the DataNodes.\n",
      "3. Set up alerts for specific conditions that indicate potential data corruption, such as high levels of replication errors or unexpected data transfer rates between DataNodes.\n",
      "4. Utilize tools like Balance, fsck, and hdfs haadmin to periodically verify the integrity of your Hadoop cluster.\n",
      "5. Implement proper backup strategies to minimize the impact of data loss or corruption incidents.\n",
      "\n",
      "By employing these best practices, you will be better equipped to identify and address unusual block verification errors when they occur.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Detect unusual block verification errors from DataNodes.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5666ebcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Analyze fullscreen and docked mode UI transitions. (Intent: semantic | FCAPS: Performance | Risk: Low)\n",
      "[Tool:semantic] Logs sent to LLM:\n",
      " - setSystemUiVisibility vis=708 mask=ffffffff oldVal=40000600 newVal=708 diff=40000108 fullscreenStackVis=0 dockedStackVis=0, fullscreenStackBounds=Rect(0, 0 - 720, 1280), dockedStackBounds=Rect(0, 0 - 0, 0)\n",
      "- setSystemUiVisibility vis=80000508 mask=ffffffff oldVal=c0000700 newVal=80000508 diff=40000208 fullscreenStackVis=0 dockedStackVis=0, fullscreenStackBounds=Rect(0, 0 - 720, 1280), dockedStackBounds=Rect(0, 0 - 0, 0)\n",
      "Intent → Semantic | FCAPS → Performance | Risk → Low\n",
      "\n",
      "\n",
      "Based on the provided logs, it appears that there have been two system UI visibility changes, both involving the setting of the `vis` property to specific values and the `mask` property being set to `ffffffff`. This indicates that these logs relate to changes in the system UI visibility for an application.\n",
      "\n",
      "The first log entry shows a change from `vis=40000600` to `vis=708`, with a difference value of `40000108`. The `mask` value remains unchanged, indicating that this change does not involve any flags being added or removed. The `fullscreenStackVis` and `dockedStackVis` properties are also set to `0` for both entries, suggesting that neither the fullscreen nor docked stacks were affected by this change. However, the `fullscreenStackBounds` and `dockedStackBounds` properties do contain information about the bounds of the application in each stack. In this case, the fullscreen bounds are set to `Rect(0, 0 - 720, 1280)`, while the docked bounds are set to `Rect(0, 0 - 0, 0)`. This suggests that the application was previously in a fullscreen state with dimensions of 720x1280, but after the change, it has transitioned to a docked state with no visible bounds.\n",
      "\n",
      "The second log entry shows a change from `vis=c0000700` to `vis=80000508`, with a difference value of `40000208`. Again, the `mask` value remains unchanged, and both `fullscreenStackVis` and `dockedStackVis` are set to `0`. The `fullscreenStackBounds` and `dockedStackBounds` properties remain the same as in the first log entry. This suggests that the application's UI state did not change between these two events, and that they likely represent separate UI transitions.\n",
      "\n",
      "In summary, based on the provided logs, we can infer that there have been two UI transitions for the application, one from a fullscreen state to a docked state, and another that did not involve a change in UI state. The logs do not provide enough information to determine the cause of these transitions or whether they were initiated by the user or the system. Further analysis would be required to determine the underlying reasons for these UI transitions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Analyze fullscreen and docked mode UI transitions.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cb987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMPLEX QUERIES = MULTISTEP REASONING, CROSS-DATASET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df1908e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Detect remote SSH brute force attempts and check if any block replica issues occurred afterward. (Intent: time | FCAPS: Security | Risk: Medium)\n",
      "[Tool:time] Logs sent to LLM:\n",
      " - authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=210.76.59.29  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=massive.merukuru.org\n",
      "Intent → Time | FCAPS → Security | Risk → Medium\n",
      "\n",
      "Based on the provided logs, it appears that there have been multiple failed SSH authentication attempts originating from external sources. The following details summarize the findings:\n",
      "\n",
      "1. Three separate authentication failures were detected in the logs, all involving unsuccessful SSH connections.\n",
      "2. The source IP addresses of these failed attempts were: 210.76.59.29, 218.188.2.4, and massive.merukuru.org.\n",
      "3. To determine if these events represent potential brute force attacks, you should investigate the frequency and pattern of these attempts. Checking the system logs for a prolonged period can help identify if there is an increase in such attempts or if they occur during specific hours.\n",
      "4. Additionally, you may want to review the server's access control lists and firewall rules to ensure that only authorized IP addresses are allowed to connect via SSH. Implementing rate limiting or other protective measures can also help mitigate the risk of brute force attacks.\n",
      "5. If you suspect that these failed attempts are related to block replica issues, you should examine the system logs around the time of these events for any relevant error messages or warnings. Look for entries related to database connections, file systems, or other components that might be affected by replica issues.\n",
      "6. Analyzing the logs for any critical time-based issues requires a more comprehensive analysis of the entire log dataset. Tools like Elasticsearch, Kibana, or other log analytics platforms can help you visualize the data and identify trends or anomalies. This could include analyzing the distribution of failed attempts throughout the day, week, or month, as well as identifying any correlation between these events and other system activities or external factors.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Detect remote SSH brute force attempts and check if any block replica issues occurred afterward.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8010e8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Correlate job resource allocation denials with HDFS packet delays. (Intent: semantic | FCAPS: Fault | Risk: Low)\n",
      " No matching dataset found, falling back to all logs\n",
      " No matching dataset found, falling back to all logs\n",
      "[Tool:semantic] Logs sent to LLM:\n",
      " - Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 192 seconds.  Will retry shortly ...\n",
      "- Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 338 seconds.  Will retry shortly ...\n",
      "Intent → Semantic | FCAPS → Fault | Risk → Low\n",
      "\n",
      "\n",
      "Based on the provided logs, it appears that there is an issue related to the renewal of leases for a specific DFS client named [DFSClient_NONMAPREDUCE_1537864556_1]. The logs indicate that the client was unable to renew its lease for accessing the HDFS resources for durations of 192 seconds and 338 seconds respectively. This could potentially be indicative of resource contention or network congestion leading to packet delays.\n",
      "\n",
      "To further investigate this issue, we would need to gather more information from various sources such as:\n",
      "\n",
      "1. HDFS NameNode logs: Analyzing the NameNode logs can provide insights into any ongoing operations, errors, or resource utilization patterns that may impact the performance of the HDFS cluster. We can look for any signs of high load, resource contention, or network issues that might be causing packet delays.\n",
      "\n",
      "2. YARN ResourceManager logs: YARN manages the resource allocation for MapReduce jobs and other applications running in Hadoop clusters. By examining the ResourceManager logs, we can identify any resource allocation denials or contention events that might correlate with the observed packet delays.\n",
      "\n",
      "3. Network statistics: Examining network statistics such as throughput, latency, and packet loss can help us determine whether network congestion is contributing to the packet delays. Tools like netstat, tcpdump, or Wireshark can be used to collect this data.\n",
      "\n",
      "4. Job configuration and execution details: Understanding the specific configurations and execution details of the MapReduce jobs running during the observed packet delays can provide valuable context. For example, large input sizes, complex data processing tasks, or misconfigured job settings could lead to increased resource demands and subsequent packet delays.\n",
      "\n",
      "Based on the available information from the logs alone, it's not possible to definitively diagnose the root cause of the issue. However, based on the correlation between the failed lease renewals and potential packet delays, it seems reasonable to assume that resource contention or network congestion might be contributing factors. Further investigation using the methods outlined above should help clarify the situation and guide appropriate remediation efforts.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Correlate job resource allocation denials with HDFS packet delays.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d9f9a35",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Identify nova compute failures caused by libvirt instance storage problems. (Intent: semantic | FCAPS: Fault | Risk: Low)\n",
      "[Tool:semantic] Logs sent to LLM:\n",
      " - image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage\n",
      "- image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage\n",
      "Intent → Semantic | FCAPS → Fault | Risk → Low\n",
      "\n",
      "\n",
      "Based on the provided logs, it appears that there are no current libvirt instance storage issues causing Nova compute failures. The logs indicate that the specified image (0673dd71-34c5-4fbb-86c4-40623fbe45b4) is currently in use locally on the node, with no instances using the shared instance storage for this image.\n",
      "\n",
      "However, it's important to note that these logs do not necessarily rule out potential past or future libvirt instance storage issues. They simply don't provide any evidence of such issues at the moment.\n",
      "\n",
      "To proactively monitor and identify Nova compute failures caused by libvirt instance storage problems, you may consider implementing the following measures:\n",
      "\n",
      "1. Regularly check the Nova and Libvirt logs for error messages related to instance storage. This can be done manually or through automated monitoring tools.\n",
      "\n",
      "2. Set up alerts for specific error messages or patterns in the logs that could indicate instance storage issues. For example, errors related to LVM, iSCSI, or other storage technologies used by OpenStack.\n",
      "\n",
      "3. Monitor the performance and availability of your instance storage systems, such as SANs, NAS, or local disks. Use tools like Nagios, Icinga, or Zabbix to monitor capacity, latency, and other key performance indicators.\n",
      "\n",
      "4. Ensure that your OpenStack environment is configured correctly for instance storage. This includes proper configuration of the storage backends, volume drivers, and other components involved in managing instance storage.\n",
      "\n",
      "5. Keep your OpenStack and underlying storage infrastructure software up-to-date with the latest patches and releases. This can help mitigate known vulnerabilities and bugs that could lead to instance storage issues.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Identify nova compute failures caused by libvirt instance storage problems.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd531dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Correlate UI responsiveness metrics with SystemUI service crashes. (Intent: semantic | FCAPS: Fault | Risk: Low)\n",
      "[Tool:semantic] Logs sent to LLM:\n",
      " - release:lock=62617001, flg=0x0, tag=\"WindowManager\", name=android\", ws=WorkSource{10113}, uid=1000, pid=1702\n",
      "- release:lock=189667585, flg=0x0, tag=\"*launch*\", name=android\", ws=WorkSource{10111}, uid=1000, pid=1702\n",
      "- release:lock=189667585, flg=0x0, tag=\"*launch*\", name=android\", ws=WorkSource{10111}, uid=1000, pid=1702\n",
      "- release:lock=62617001, flg=0x0, tag=\"WindowManager\", name=android\", ws=WorkSource{10113}, uid=1000, pid=1702\n",
      "Intent → Semantic | FCAPS → Fault | Risk → Low\n",
      "\n",
      "\n",
      "Based on the provided logs, it appears that there are two distinct events occurring: window manager lock acquisitions and system UI service launches. Both events involve the same user ID (1000), process ID (1702), and work source IDs (10111 and 10113).\n",
      "\n",
      "The first observation to make is that the window manager locks and system UI service launches seem to be interleaved. This suggests that the system UI service might be involved in managing windows or handling UI interactions.\n",
      "\n",
      "To correlate UI responsiveness metrics with system UI service crashes, we would need additional data such as UI frame rates, touch input latency, GPU usage, CPU usage, and system UI service crash logs. However, based on the available information, we can infer some possible relationships between these events:\n",
      "\n",
      "1. The system UI service might be responsible for managing the UI of the android application during these window manager lock acquisitions. If the system UI service crashes while holding a window manager lock, it could potentially lead to UI freezes or unresponsiveness.\n",
      "2. The frequent acquisition of window manager locks by the system UI service might indicate heavy UI rendering or interaction, which could put additional strain on the system UI service and increase the likelihood of crashes.\n",
      "3. It's also possible that the system UI service crashes are causing the window manager locks rather than the other way around. In this case, the window manager might be attempting to acquire a lock on a resource that is no longer available due to the crashed system UI service.\n",
      "4. Analyzing the frequency and duration of window manager locks and system UI service launches could provide insights into potential performance bottlenecks or issues that may be contributing to both UI unresponsiveness and system UI service crashes.\n",
      "\n",
      "To further investigate these hypotheses, you could consider collecting more detailed performance metrics and system logs, specifically focusing on UI rendering, touch input processing, and system UI service crashes. Additionally, analyzing any patterns or trends in the data could help identify underlying causes and potential solutions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Correlate UI responsiveness metrics with SystemUI service crashes.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "943335eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Check if any Linux login failures led to OpenStack API unauthorized access events. (Intent: fcaps | FCAPS: Security | Risk: Medium)\n",
      "[Tool:fcaps] Logs sent to LLM:\n",
      " - 10.11.21.131,10.11.10.1 \"GET /openstack/2013-10-17/vendor_data.json HTTP/1.1\" status: 200 len: 124 time: 0.2301619\n",
      "- 10.11.21.124,10.11.10.1 \"GET /openstack/2013-10-17/user_data HTTP/1.1\" status: 404 len: 176 time: 0.0010281\n",
      "- 10.11.21.139,10.11.10.1 \"GET /openstack/2013-10-17/vendor_data.json HTTP/1.1\" status: 200 len: 124 time: 0.2188199\n",
      "- 10.11.21.134,10.11.10.1 \"GET /openstack/2013-10-17/user_data HTTP/1.1\" status: 404 len: 176 time: 0.2255361\n",
      "Intent → Fcaps | FCAPS → Security | Risk → Medium\n",
      "\n",
      "\n",
      "Based on the provided logs, there are no explicit Linux login failure records mentioned. However, we can infer some potential security concerns from the given data related to OpenStack API unauthorized access attempts.\n",
      "\n",
      "The logs show two separate instances where clients with IP addresses 10.11.21.131 and 10.11.21.139 successfully accessed the OpenStack vendor_data.json file while clients with IP addresses 10.11.21.124 and 10.11.21.134 failed to access the user_data file, both resulting in 404 Not Found errors.\n",
      "\n",
      "Although there's no evidence of Linux login failures directly linked to these OpenStack API unauthorized access attempts, it is essential to consider that an attacker might have gained unauthorized access through other means such as weak passwords, exploited vulnerabilities, or stolen credentials.\n",
      "\n",
      "To mitigate this risk, you may consider implementing the following measures:\n",
      "\n",
      "1. Enforce strong password policies: Ensure all users adhere to strong password requirements, including complexity rules and regular password changes.\n",
      "2. Implement multi-factor authentication (MFA): Enable MFA for all OpenStack API access to add an extra layer of security.\n",
      "3. Regularly update OpenStack software components: Keep your OpenStack environment up-to-date with the latest patches and security updates to minimize the risk of known vulnerabilities being exploited.\n",
      "4. Monitor and analyze OpenStack API access logs: Continuously monitor and analyze OpenStack API access logs to identify any suspicious activity, such as repeated failed access attempts or unusual patterns.\n",
      "5. Implement network segmentation: Segment your network to limit the exposure of sensitive resources like OpenStack APIs to only trusted networks and hosts.\n",
      "6. Use encryption: Encrypt sensitive data transmitted over the network or stored within OpenStack to protect against data breaches.\n",
      "7. Implement role-based access control: Assign appropriate roles and permissions to users based on their job responsibilities and ensure they follow the principle of least privilege.\n",
      "\n",
      "By implementing these measures, you can significantly reduce the risk of unauthorized access to your OpenStack environment and enhance overall security.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Check if any Linux login failures led to OpenStack API unauthorized access events.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bc647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FCAPS-Focused queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20af0538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Predict future SSH authentication failures that could escalate to root compromise. (Intent: fcaps | FCAPS: Security | Risk: Medium)\n",
      "[Tool:fcaps] Logs sent to LLM:\n",
      " - authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root\n",
      "Intent → Fcaps | FCAPS → Security | Risk → Medium\n",
      "\n",
      "\n",
      "Based on the provided logs, it appears there have been two failed SSH authentication attempts from the same IP address (207.243.167.114) targeting the root user account. This type of activity can be indicative of a potential brute force attack or an attempt to gain unauthorized access to the system as the root user.\n",
      "\n",
      "In the context of FCAPS security framework, this event falls under the \"Authentication\" category since it involves failed login attempts. The risk level associated with these events can be considered medium due to the following reasons:\n",
      "\n",
      "1. Multiple failed attempts targeting the root user account from the same IP address within a short time frame increases the likelihood of a successful attack.\n",
      "2. Successful unauthorized access as the root user would grant full control over the system, potentially leading to significant damage or data loss.\n",
      "\n",
      "To mitigate the risk of further attacks or potential compromise, consider implementing the following measures:\n",
      "\n",
      "1. Implement strong password policies for all users, especially those with elevated privileges like root. Enforce regular password changes and complexity requirements.\n",
      "2. Enable Multi-Factor Authentication (MFA) for critical accounts such as root. MFA adds an extra layer of security by requiring additional verification methods beyond just a password.\n",
      "3. Monitor and limit failed login attempts. Set up alerts for multiple failed attempts originating from the same IP address or source port.\n",
      "4. Block IP addresses after a certain number of failed attempts. This can help prevent brute force attacks and reduce the risk of unauthorized access.\n",
      "5. Regularly review system logs for suspicious activities and investigate any anomalous behavior promptly.\n",
      "6. Consider implementing a network segmentation strategy to isolate critical systems and limit the attack surface.\n",
      "7. Keep your operating system and applications updated with the latest security patches.\n",
      "8. Educate users about safe computing practices and the importance of not sharing their credentials.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Predict future SSH authentication failures that could escalate to root compromise.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "874ceb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - List potential misconfigurations in Hadoop YARN resource allocations. (Intent: fcaps | FCAPS: Configuration | Risk: Low)\n",
      "[Tool:fcaps] Logs sent to LLM:\n",
      " - Extract jar:file:/D:/hadoop-2.6.0-localbox/share/hadoop/yarn/hadoop-yarn-common-2.6.0-SNAPSHOT.jar!/webapps/mapreduce to C:\\Users\\msrabi\\AppData\\Local\\Temp\\Jetty_0_0_0_0_62267_mapreduce____.8n7xum\\webapp\n",
      "- Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context static\n",
      "- The job-conf file on the remote FS is /tmp/hadoop-yarn/staging/msrabi/.staging/job_1445144423722_0020/job.xml\n",
      "- loaded properties from hadoop-metrics2.properties\n",
      "- Event Writer setup for JobId: job_1445144423722_0020, File: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445144423722_0020/job_1445144423722_0020_1.jhist\n",
      "Intent → Fcaps | FCAPS → Configuration | Risk → Low\n",
      "\n",
      "\n",
      "Based on the provided logs, it appears that there is a MapReduce application running on Hadoop YARN with the ID 'job_1445144423722_0020'. The logs indicate some configuration changes related to this application. Let's analyze these configurations under the FCAPS domain:\n",
      "\n",
      "FCAPS Domain: Configuration\n",
      "\n",
      "1. **File System Location:** The job configuration file is stored on the remote filesystem at '/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445144423722_0020/job.xml'. This location is within the staging directory, which is typically used for temporary data storage during job execution. It's not necessarily a misconfiguration, but it might be worth considering moving the job configuration file to a more permanent location for easier access and version control.\n",
      "\n",
      "Risk Level: Low\n",
      "Mitigation: Monitor the performance of your Hadoop cluster and consider moving the job configuration files to a more permanent location if necessary.\n",
      "\n",
      "2. **Web Proxy Filter:** A new web proxy filter named 'AM_PROXY_FILTER' has been added to the context static. This filter is of class 'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'. The purpose of this filter is to restrict access to Application Masters based on IP addresses. While this is not inherently a misconfiguration, it could potentially limit the functionality or availability of your Hadoop cluster if not configured properly. Make sure you have a clear understanding of how this filter works and that it is set up correctly to meet your security requirements.\n",
      "\n",
      "Risk Level: Medium\n",
      "Mitigation: Review the configuration of the 'AM_PROXY_FILTER' and ensure it is set up correctly to allow access only from trusted sources while still allowing legitimate users to run jobs.\n",
      "\n",
      "3. **Event Writer Setup:** An event writer has been set up for this job with a file located at 'hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445144423722_0020/job_1445144423722_0020_1.jhist'. This file will contain job history information. There doesn't seem to be anything wrong with this setup, but it's important to make sure that the HDFS location where these files are being written has sufficient space and is accessible by all nodes in the cluster.\n",
      "\n",
      "Risk Level: Low\n",
      "Mitigation: Monitor the available space on the HDFS location where these job history files are being written and ensure that it remains sufficient.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"List potential misconfigurations in Hadoop YARN resource allocations.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d93611a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Analyze API usage counts in OpenStack that could indicate billing anomalies. (Intent: semantic | FCAPS: Configuration | Risk: Low)\n",
      "[Tool:semantic] Logs sent to LLM:\n",
      " - 10.11.21.138,10.11.10.1 \"GET /openstack/2013-10-17/vendor_data.json HTTP/1.1\" status: 200 len: 124 time: 0.2222931\n",
      "- 10.11.21.137,10.11.10.1 \"GET /openstack/2013-10-17/vendor_data.json HTTP/1.1\" status: 200 len: 124 time: 0.0005460\n",
      "Intent → Semantic | FCAPS → Configuration | Risk → Low\n",
      "\n",
      "Based on the provided logs, we have two successful GET requests to the \"/openstack/2013-10-17/vendor_data.json\" endpoint from different IP addresses (10.11.21.138 and 10.11.21.137) within the same day (October 17, 2013). Both requests resulted in a 200 status code and had identical response lengths of 124 bytes. The difference in request processing times (0.2222931 seconds for the first request and 0.0005460 seconds for the second request) might be due to network latency or server load variations.\n",
      "\n",
      "To analyze API usage counts in OpenStack that could indicate billing anomalies, we would need more data points and context. Here are some suggestions for further investigation:\n",
      "\n",
      "1. Check the access control logs to determine if these requests were made by authorized users or applications. Unauthorized API usage can lead to unexpected charges.\n",
      "2. Review the API usage patterns for these IP addresses over an extended period. Sudden spikes or unusual usage patterns could indicate billing anomalies.\n",
      "3. Examine the size and complexity of the requested JSON data. Large or complex requests may consume more resources and result in higher costs.\n",
      "4. Analyze the timing of the requests. Peak hours or specific days might have higher pricing tiers.\n",
      "5. Investigate any correlation between these requests and other system events, such as infrastructure changes or software upgrades.\n",
      "\n",
      "In conclusion, based on the given logs, there is no clear indication of billing anomalies. However, further analysis is required to validate this assumption and ensure accurate cost allocation in the OpenStack environment.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Analyze API usage counts in OpenStack that could indicate billing anomalies.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91cf8fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Identify slow job completions and recommend tuning parameters in Hadoop. (Intent: semantic | FCAPS: Configuration | Risk: Low)\n",
      "[Tool:semantic] Logs sent to LLM:\n",
      " - Reduce slow start threshold not met. completedMapsForReduceSlowstart 1\n",
      "- Reduce slow start threshold not met. completedMapsForReduceSlowstart 1\n",
      "Intent → Semantic | FCAPS → Configuration | Risk → Low\n",
      "\n",
      "\n",
      "Based on the provided logs, it appears that the Hadoop MapReduce jobs have failed to meet the slow start threshold for reduction tasks. The slow start threshold is a mechanism used by Hadoop to delay the initiation of reducer tasks until enough map outputs have been received. This helps to balance the workload between mappers and reducers and avoid unnecessary resource utilization.\n",
      "\n",
      "The logs indicate that this threshold has not been met twice, suggesting that there may be an issue causing the map tasks to complete too quickly, resulting in a surplus of map outputs before the reducers have had a chance to start. This could lead to inefficient use of resources and potentially longer overall job completion times due to the need to process these excess map outputs.\n",
      "\n",
      "To address this issue, several tuning parameters can be considered:\n",
      "\n",
      "1. Increase the number of map tasks per job: By increasing the number of map tasks, more data can be processed in parallel, reducing the time taken for each individual map task to complete. This should help ensure that the slow start threshold is met and that reducers are initiated in a timely manner.\n",
      "\n",
      "2. Adjust the input split size: Larger input splits can result in fewer map tasks being created, which may help prevent the issue of excess map outputs. However, larger splits can also increase memory usage and may require more resources. It's important to find a balance between the number of map tasks and the size of the input splits to optimize performance.\n",
      "\n",
      "3. Tune the combiner function: A combiner function can be used to combine intermediate key-value pairs produced by map tasks before they reach the reducers. This can help reduce the amount of data that needs to be transferred between mappers and reducers, potentially improving overall job performance.\n",
      "\n",
      "4. Configure the I/O buffer sizes: Properly configuring the I/O buffer sizes for both the mapper and reducer tasks can help improve data transfer efficiency and reduce the time taken for map tasks to complete. This can help ensure that the slow start threshold is met and that reducers are initiated when expected.\n",
      "\n",
      "5. Monitor and adjust the YARN resource manager settings: The YARN resource manager can be configured to allocate resources differently based on the specific requirements of the job. Tuning these settings, such as the amount of memory reserved for the application master or the number of containers allocated, can help optimize resource utilization and ensure that the slow start threshold is consistently met.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Identify slow job completions and recommend tuning parameters in Hadoop.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22543545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Detect repeated system login failures hinting at brute-force attempts. (Intent: cluster | FCAPS: Security | Risk: Medium)\n",
      "[Tool:cluster] Logs sent to LLM:\n",
      " - authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.214.161.141  user=root\n",
      "Intent → Cluster | FCAPS → Security | Risk → Medium\n",
      "\n",
      "Based on the provided logs, it appears that there have been two failed authentication attempts from two different IP addresses (207.243.167.114 and 211.214.161.141) targeting the root user via SSH. These events could potentially indicate brute-force attacks as they involve multiple unsuccessful attempts to gain access to the system using the same user account (root). To further investigate this issue, you may consider implementing the following actions:\n",
      "1. Review the system's firewall rules to ensure that only authorized IP addresses are allowed to connect via SSH.\n",
      "2. Implement rate limiting or throttling for SSH connections to prevent excessive login attempts.\n",
      "3. Enable fail2ban or another intrusion prevention solution to automatically block IP addresses after a certain number of failed login attempts.\n",
      "4. Monitor the system for any other unusual login activity, such as failed attempts targeting other user accounts or successful logins from unexpected locations.\n",
      "5. Consider implementing multi-factor authentication to add an additional layer of security to the system.\n",
      "6. Regularly review and update the system's software and configurations to address any known vulnerabilities.\n",
      "7. Conduct regular security awareness training for users to help them understand the importance of strong passwords and the risks associated with phishing attacks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Detect repeated system login failures hinting at brute-force attempts.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58bb186",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREDICTIVE/ MITIGATION QUERIES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f82ab19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Based on past login attempts, predict next possible vulnerable user accounts. (Intent: semantic | FCAPS: Security | Risk: Medium)\n",
      "[Tool:semantic] Logs sent to LLM:\n",
      " - check pass; user unknown\n",
      "- check pass; user unknown\n",
      "Intent → Semantic | FCAPS → Security | Risk → Medium\n",
      "\n",
      "\n",
      "Given the two log entries provided, it appears that an unidentified user is attempting to gain access to the system using invalid credentials. The repeated occurrence of \"user unknown\" suggests that these login attempts are likely from unauthorized users. While we cannot definitively determine which specific user accounts may be at risk based solely on this information, there are several reasonable inferences that can be made.\n",
      "\n",
      "First, since the same error message (\"check pass; user unknown\") is present in both logs, it's likely that the attacker is targeting multiple user accounts with the same incorrect credentials. This could indicate that the attacker has obtained a list of potential usernames or email addresses and is systematically trying different password combinations for each one.\n",
      "\n",
      "Second, the fact that the usernames are unknown implies that they may not be easily guessable or commonly used. Attackers often use automated tools to brute force common usernames and passwords, so the absence of such attempts in these logs suggests that the targeted accounts might be less obvious choices.\n",
      "\n",
      "Third, it's important to note that these login attempts do not necessarily mean that the corresponding user accounts have been compromised. It's possible that the attacker is simply guessing randomly or using a dictionary attack. However, repeated failed login attempts can still put additional stress on the system and potentially trigger account lockouts or other security measures.\n",
      "\n",
      "To mitigate the risk of unauthorized access, it's recommended to implement the following best practices:\n",
      "\n",
      "1. Enable multi-factor authentication (MFA) for all user accounts to add an extra layer of security beyond just a username and password.\n",
      "2. Implement rate limiting and account lockout policies to prevent brute force attacks and limit the number of failed login attempts per user.\n",
      "3. Regularly review and monitor login activity to identify any unusual patterns or suspicious behavior.\n",
      "4. Educate users about the importance of strong passwords and the risks associated with weak or reused credentials.\n",
      "5. Consider implementing a centralized identity and access management solution to help manage and secure user identities across your organization.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Based on past login attempts, predict next possible vulnerable user accounts.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d2bc499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Forecast container resource starvation risks based on current event patterns. (Intent: cluster | FCAPS: Configuration | Risk: Low)\n",
      "[Tool:cluster] Logs sent to LLM:\n",
      " - Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445144423722_0020_01_000003 taskAttempt attempt_1445144423722_0020_m_000001_0\n",
      "- Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445144423722_0020_01_000005 taskAttempt attempt_1445144423722_0020_m_000003_0\n",
      "- Container complete event for unknown container id container_1445144423722_0020_01_000012\n",
      "- Got allocated containers 2\n",
      "- Diagnostics report from attempt_1445144423722_0020_m_000003_0: Container killed by the ApplicationMaster.\n",
      "Intent → Cluster | FCAPS → Configuration | Risk → Low\n",
      "\n",
      "Based on the provided logs, it appears that there is an issue with container resource management in the specified taskAttempts. Specifically, we see two events related to container cleanup and launch, followed by a container completion event with an unknown container ID, and a diagnostics report indicating that one container was killed by the ApplicationMaster.\n",
      "\n",
      "To forecast potential container resource starvation risks, let's analyze these events in more detail:\n",
      "\n",
      "1. The first event indicates a remote cleanup of container container_1445144423722_0020_01_000003, which suggests that this container may have been terminated prematurely due to resource constraints. This could potentially indicate resource starvation for this container.\n",
      "\n",
      "2. The second event shows the launch of a new container, container_1445144423722_0020_01_000005, for the same taskAttempt. This could be a response to the first container being terminated due to resource constraints. However, it's important to note that launching a new container takes time and resources, so this may not be an ideal solution.\n",
      "\n",
      "3. The third event is a container completion event for an unknown container id, container_1445144423722_0020_01_000012. It's unclear whether this container was successfully completed or if it too experienced resource issues and was terminated.\n",
      "\n",
      "4. The fourth event indicates that two containers were allocated for this taskAttempt. Given the previous events, it seems likely that these containers were both affected by resource constraints.\n",
      "\n",
      "5. The fifth event is a diagnostics report from attempt_1445144423722_0020_m_000003_0, indicating that one container was killed by the ApplicationMaster. This further supports the hypothesis that resource starvation is occurring in this taskAttempt.\n",
      "\n",
      "To mitigate potential container resource starvation risks, consider implementing the following actions:\n",
      "\n",
      "1. Monitor container resource usage closely and set appropriate resource limits to prevent containers from consuming more than their fair share.\n",
      "2. Use container prioritization to ensure that critical containers receive sufficient resources.\n",
      "3. Implement container auto-scaling to automatically add or remove containers based on demand and available resources.\n",
      "4. Investigate any underlying causes of resource contention or misconfiguration that may be contributing to container resource starvation.\n",
      "5. Consider using container orchestration tools like Kubernetes or Docker Swarm to manage container deployment and scaling more effectively.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Forecast container resource starvation risks based on current event patterns.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b7703c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Predict potential packet transmission failures based on packet delay patterns. (Intent: cluster | FCAPS: Security | Risk: Low)\n",
      "[Tool:cluster] Logs sent to LLM:\n",
      " - connection from 67.95.49.172 () at Fri Jul 22 19:29:09 2005\n",
      "- connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul 17 15:09:15 2005\n",
      "- connection from 67.95.49.172 () at Fri Jul 22 19:29:10 2005\n",
      "- connection from 218.69.108.57 () at Fri Jun 24 18:55:11 2005\n",
      "Intent → Cluster | FCAPS → Security | Risk → Low\n",
      "\n",
      "Based on the provided logs, there appear to be two distinct IP addresses (67.95.49.172 and 203.101.45.59) making connections to the server at different times. The first IP address, 67.95.49.172, has made two connections on the same day (July 22, 2005), while the second IP address, 203.101.45.59, made a connection on July 17, 2005.\n",
      "\n",
      "To predict potential packet transmission failures, we need to analyze packet delay patterns. Unfortunately, the given logs do not provide sufficient information about packet sizes, transmission times, or other relevant metrics for accurate analysis. However, we can still identify some trends that might indicate potential issues.\n",
      "\n",
      "First, let's examine the frequency of connections from each IP address:\n",
      "- IP address 67.95.49.172: Two connections on the same day (July 22, 2005).\n",
      "- IP address 203.101.45.59: One connection on a different day (July 17, 2005).\n",
      "\n",
      "While this data doesn't directly relate to packet delay patterns, it could potentially indicate network congestion or other issues that may affect packet transmission. For example, if these IP addresses belong to users in the same geographic region or are attempting to access the same resource, high traffic volumes could lead to increased latency and potential packet loss.\n",
      "\n",
      "To further investigate potential packet transmission failures, you would need to collect more detailed information about packet sizes, transmission times, and error rates. This data can help you identify specific patterns or anomalies that may indicate packet loss or delayed transmissions. Additionally, analyzing traffic patterns over extended periods can help you identify trends and correlations that might not be apparent from individual log entries.\n",
      "\n",
      "In summary, based on the limited information provided in the logs, there are two distinct IP addresses making connections to the server at different times. While we cannot definitively predict potential packet transmission failures without additional data, the frequency of connections from one IP address and the absence of connections from another IP address suggest that network congestion or other issues may be impacting packet transmission. To gain a better understanding of packet delay patterns and potential failures, you should collect more detailed information about packet sizes, transmission times, and error rates.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Predict potential packet transmission failures based on packet delay patterns.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b4748fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Predict future UI freezes if fullscreen visibility transitions remain inconsistent. (Intent: semantic | FCAPS: Fault | Risk: Low)\n",
      "[Tool:semantic] Logs sent to LLM:\n",
      " - setSystemUiVisibility vis=0 mask=1 oldVal=40000500 newVal=40000500 diff=0 fullscreenStackVis=0 dockedStackVis=0, fullscreenStackBounds=Rect(0, 0 - 0, 0), dockedStackBounds=Rect(0, 0 - 0, 0)\n",
      "- setSystemUiVisibility vis=708 mask=ffffffff oldVal=40000600 newVal=708 diff=40000108 fullscreenStackVis=0 dockedStackVis=0, fullscreenStackBounds=Rect(0, 0 - 720, 1280), dockedStackBounds=Rect(0, 0 - 0, 0)\n",
      "Intent → Semantic | FCAPS → Fault | Risk → Low\n",
      "\n",
      "\n",
      "Based on the provided logs, we can observe that there are two events related to setting system UI visibility. Both events have the same \"vis\" value before and after the change, indicating no visible change in the UI. However, the \"mask\" value differs significantly between the two events.\n",
      "\n",
      "The first event sets the \"mask\" value to 1, which corresponds to the SYSTEM_UI_FLAG_LOW_PROFILE flag. This flag is used to hide the status bar and navigation bar when an application is in low power mode. The other flags, such as fullscreenStackVis and dockedStackVis, are set to 0 for both events, meaning they are hidden.\n",
      "\n",
      "The second event sets the \"mask\" value to 0xffffffff, which is equivalent to setting all flags. The \"vis\" value is also different, now set to 708, which corresponds to the SYSTEM_UI_FLAG_HIDE_NAVIGATION | SYSTEM_UI_FLAG_FULLSCREEN flags. These flags are used to hide the navigation bar and make the application take up the entire screen. The fullscreenStackBounds and dockedStackBounds values have changed as well, indicating that the layout of the UI has been adjusted accordingly.\n",
      "\n",
      "Although these events do not directly indicate UI freezes, they do suggest inconsistencies in how the fullscreen visibility transitions are being handled by the system. It's possible that these transitions could lead to performance issues or UI glitches, potentially resulting in perceived freezes.\n",
      "\n",
      "To predict future UI freezes, it would be helpful to analyze a larger dataset of similar events and look for patterns or trends. For example, if certain applications consistently exhibit these types of transitions and frequently result in UI freezes, then it may be worth investigating further. Additionally, monitoring the system resources and CPU usage during these transitions could provide insight into any performance bottlenecks that might be contributing to the issue.\n",
      "\n",
      "In summary, while the provided logs do not definitively prove that future UI freezes will occur due to inconsistent fullscreen visibility transitions, they do raise concerns based on past behavior. Further analysis and monitoring are recommended to determine if this is a widespread issue and to identify potential causes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Predict future UI freezes if fullscreen visibility transitions remain inconsistent.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3630d596",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Predict future UI freezes if fullscreen visibility transitions remain inconsistent. (Intent: semantic | FCAPS: Fault | Risk: Low)\n",
      "[Tool:semantic] Logs sent to LLM:\n",
      " - setSystemUiVisibility vis=0 mask=1 oldVal=40000500 newVal=40000500 diff=0 fullscreenStackVis=0 dockedStackVis=0, fullscreenStackBounds=Rect(0, 0 - 0, 0), dockedStackBounds=Rect(0, 0 - 0, 0)\n",
      "- setSystemUiVisibility vis=708 mask=ffffffff oldVal=40000600 newVal=708 diff=40000108 fullscreenStackVis=0 dockedStackVis=0, fullscreenStackBounds=Rect(0, 0 - 720, 1280), dockedStackBounds=Rect(0, 0 - 0, 0)\n",
      "Intent → Semantic | FCAPS → Fault | Risk → Low\n",
      "\n",
      "\n",
      "Based on the provided logs, we can observe that the system UI visibility settings have undergone two transitions: one with no change (visibility value remained the same), and another with a significant difference (visibility mask value changed). Let's examine each event in detail to understand their implications for potential UI freezes.\n",
      "\n",
      "Event 1:\n",
      "The first event shows no change in the visibility value, which means the fullscreen and docked stacks remained unchanged. Since there is no visible difference between the old and new values, it is unlikely that this transition would cause any UI freezes. However, it is essential to note that other factors not captured in these logs could still contribute to UI freezes. For instance, resource intensive operations occurring during this transition might lead to performance issues.\n",
      "\n",
      "Event 2:\n",
      "In the second event, the visibility mask value has significantly changed from 40000600 to 708. This indicates that several individual visibility flags have been toggled. Specifically, the 'fullscreen' flag was enabled (mask value 708 includes this flag), while the 'docked' flag was disabled (mask value 40000600 included this flag). The new bounds for the fullscreen stack also suggest that the entire screen width (1280 pixels) is now being used for the fullscreen UI.\n",
      "\n",
      "This transition could potentially impact UI performance due to the following reasons:\n",
      "\n",
      "1. Resource allocation: Enabling fullscreen mode requires additional resources, such as memory and processing power. If the application does not manage these resources efficiently, it may lead to UI freezes or stutters.\n",
      "\n",
      "2. Layout recalculation: Changing the UI layout from docked to fullscreen requires the application to recalculate the positioning of all elements within the fullscreen area. This process can be computationally expensive and time-consuming, leading to temporary UI freezes.\n",
      "\n",
      "3. Compositing: When switching to fullscreen mode, the operating system needs to composite the fullscreen content over other windows. This process can introduce delays and cause flickering or tearing effects, especially if the application is not optimized for fullscreen rendering.\n",
      "\n",
      "To predict future UI freezes related to fullscreen visibility transitions, we should monitor the following metrics:\n",
      "\n",
      "1. CPU usage: High CPU usage during fullscreen transitions could indicate that the application is struggling to allocate resources effectively, leading to UI freezes.\n",
      "\n",
      "2. GPU usage: Intense graphical workloads during fullscreen transitions could result in UI stutters or freezes.\n",
      "\n",
      "3. Memory usage: Rapidly increasing memory usage during fullscreen transitions could indicate memory leaks or inefficient resource management, both of which could contribute to UI freezes.\n",
      "\n",
      "4. Frame rate: Drops in frame rate during fullscreen transitions could indicate that the application is unable to keep up with the demands of rendering the fullscreen UI, leading to UI freezes or stutters.\n",
      "\n",
      "By analyzing these metrics in conjunction with the logs, we can identify trends and patterns that may indicate impending UI freezes related to fullscreen visibility transitions. If such trends are observed, it may be necessary to investigate the underlying causes and implement optimizations to improve the application's performance during these transitions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Predict future UI freezes if fullscreen visibility transitions remain inconsistent.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb6bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## VAGUE QUERIES - testing hallucination and fallback ability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08581699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Is there any suspicious activity happening recently in the systems? (Intent: semantic | FCAPS: Security | Risk: Low)\n",
      "[Tool:semantic] Logs sent to LLM:\n",
      " - connection from 211.72.151.162 () at Mon Jul 18 03:26:49 2005\n",
      "- connection from 217.187.83.50 () at Mon Jul 25 23:24:09 2005\n",
      "- connection from 222.33.90.199 () at Mon Jun 20 03:40:59 2005\n",
      "- connection from 221.4.102.93 () at Thu Jul  7 23:09:47 2005\n",
      "Intent → Semantic | FCAPS → Security | Risk → Low\n",
      "\n",
      "Based on the provided logs, there have been four connections to the system from external IP addresses within the last two months. The IP addresses are: 211.72.151.162, 217.187.83.50, 222.33.90.199, and 221.4.102.93. While this activity alone does not necessarily indicate suspicious behavior, it is worth investigating further due to the following reasons:\n",
      "\n",
      "1. The logs do not provide enough context about the nature of these connections. For example, were they successful connections, or did they result in failed login attempts? What services or ports were being accessed? This information could help determine whether the connections represent normal or abnormal activity.\n",
      "\n",
      "2. The absence of usernames or hostnames in the logs makes it difficult to associate these connections with specific users or systems. This lack of detail limits the ability to assess the potential impact of the connections and to identify any patterns or trends that might be indicative of a security threat.\n",
      "\n",
      "3. The logs span a period of over two months, which raises questions about the frequency and consistency of the monitoring and analysis efforts. Regularly reviewing and analyzing system logs can help detect and respond to security incidents more effectively.\n",
      "\n",
      "To investigate further, consider implementing additional logging and monitoring capabilities to capture more detailed information about incoming connections. This might include collecting usernames, hostnames, and other relevant data to facilitate better analysis and identification of potential threats. Additionally, consider using tools like intrusion detection systems and security information and event management (SIEM) solutions to automate the process of analyzing logs and identifying anomalous activity.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Is there any suspicious activity happening recently in the systems?\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59985259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Any pattern observed in failed operations across multiple systems? (Intent: cluster | FCAPS: Fault | Risk: Low)\n",
      "[Tool:cluster] Logs sent to LLM:\n",
      " - [instance: 7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5] VM Stopped (Lifecycle Event)\n",
      "- [instance: d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c] VM Stopped (Lifecycle Event)\n",
      "- [instance: af5f7392-f7d4-4298-b647-c98924c64aa1] VM Stopped (Lifecycle Event)\n",
      "- [instance: b562ef10-ba2d-48ae-bf4a-18666cba4a51] VM Stopped (Lifecycle Event)\n",
      "- [instance: ae3a1b5d-eec1-45bb-b76a-c59d83b1471f] VM Stopped (Lifecycle Event)\n",
      "Intent → Cluster | FCAPS → Fault | Risk → Low\n",
      "\n",
      "Based on the provided logs, it appears that all instances have experienced a \"VM Stopped\" event. This is a lifecycle event indicating that the virtual machine has been stopped for some reason. While this information alone does not provide insight into the cause of the failure, it could be an indication of a larger issue affecting multiple systems. It would be recommended to investigate further by checking the system and application logs around the time of these events to determine the root cause. Additionally, reviewing any recent configuration changes, updates, or patches applied to these instances may also provide context. If similar issues are identified across multiple systems, it could suggest a broader problem related to infrastructure or environment.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Any pattern observed in failed operations across multiple systems?\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc79156e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0946c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a57922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6383060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66954ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4367f64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38353d71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c06168fb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Identify issues with job submission or callQueue overflows in hadoop.\n",
      "Inferred datasets: ['Hadoop_2k_cleaned.csv']\n",
      " Selected cluster ID: 18\n",
      "Logs sent to LLM:\n",
      " - Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 324 seconds.  Will retry shortly ...\n",
      "- Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 351 seconds.  Will retry shortly ...\n",
      "- Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 173 seconds.  Will retry shortly ...\n",
      "- Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 354 seconds.  Will retry shortly ...\n",
      "- Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 313 seconds.  Will retry shortly ...\n",
      "Semantic → \n",
      "\n",
      "Based on the provided logs, it appears that there is an issue with a specific DFSClient (DFSClient_NONMAPREDUCE_1537864556_1) related to lease renewals. The logs indicate that the client is unable to renew its lease for a period longer than the expected time. This issue is not directly related to job submission or callQueue overflows in Hadoop. However, if this DFSClient is required for job submission or data access, the inability to renew its lease could potentially impact the performance or availability of Hadoop services. To further investigate, you may want to check the status of this DFSClient and the related NameNode or DataNode for any potential issues. Additionally, you may want to review the Hadoop logs for any related error messages or warnings. If necessary, you may need to restart the affected DFSClient or take other corrective actions.\n"
     ]
    }
   ],
   "source": [
    "query = \"Identify issues with job submission or callQueue overflows in hadoop.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa938528",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Show login failures in Linux\n",
      "Inferred datasets: ['Linux_2k_cleaned.csv']\n",
      " Selected cluster ID: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs sent to LLM:\n",
      " - authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root\n",
      " - diagnose RM heartbeats in Hadoop\n",
      "Inferred datasets: ['Hadoop_2k_cleaned.csv']\n",
      " Selected cluster ID: 25\n",
      " No logs in matching cluster, falling back to dataset-wide search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs sent to LLM:\n",
      " - Diagnostics report from attempt_1445144423722_0020_m_000001_0: Error: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "- Diagnostics report from attempt_1445144423722_0020_m_000001_0: Error: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "- Diagnostics report from attempt_1445144423722_0020_m_000002_0: Error: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "- Diagnostics report from attempt_1445144423722_0020_m_000002_0: Error: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n",
      "- Error writing History Event: org.apache.hadoop.mapreduce.jobhistory.TaskAttemptUnsuccessfulCompletionEvent@7317849d\n",
      "Semantic → \n",
      "Based on the provided logs, there have been multiple failed authentication attempts from the IP address 207.243.167.114 using SSH protocol and targeting the user account \"root\". These repeated login failures could indicate a potential brute force attack or an attempt to gain unauthorized access to the system. It is recommended to investigate further by checking the system's firewall rules, implementing stronger authentication methods, and monitoring the system for any unusual activity. Additionally, reviewing the system logs for successful logins from the same IP address can provide further context.\n",
      "Semantic → \n",
      "\n",
      "The logs indicate that there is a network connectivity issue between the client (MININT-FNANLI5) and the JobTracker (msra-sa-41) in Hadoop. Specifically, the client is unable to establish a connection to the JobTracker on the assigned port (9000) due to a \"No Route to Host\" exception.\n",
      "\n",
      "This error typically occurs when there is a network routing problem, such as a misconfigured firewall, a network partition, or a network outage. The socket timeout exception suggests that the client is trying to connect to the JobTracker, but is unable to do so within the specified timeout period.\n",
      "\n",
      "To diagnose and resolve this issue, you can try the following steps:\n",
      "\n",
      "1. Check the network connectivity between the client and the JobTracker by pinging the JobTracker from the client and vice versa. Ensure that there are no firewalls or security groups blocking the connection.\n",
      "2. Verify that the JobTracker is running and listening on the correct port (9000) by checking the JobTracker logs or by using the netstat command.\n",
      "3. Check the Hadoop configuration files, such as hdfs-site.xml and core-site.xml, to ensure that the JobTracker address and port are correctly specified.\n",
      "4. If the issue persists, you may need to consult your network administrator or check for any ongoing network issues that could be affecting the Hadoop cluster.\n",
      "5. If the issue is related to a misconfigured firewall, you may need to configure the firewall rules to allow traffic between the client and the JobTracker on port 9000.\n",
      "6. If the issue is related to a network partition, you may need to check the Hadoop cluster topology and ensure that all nodes are properly connected to each other.\n",
      "\n",
      "By following these steps, you should be able to diagnose and resolve the issue with the Hadoop RM heartbeats.\n"
     ]
    }
   ],
   "source": [
    "query = \"Show login failures in Linux and diagnose RM heartbeats in Hadoop\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd7f2188",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Find broadcast receiver triggers in Android logs.\n",
      "Inferred datasets: ['Android_2k_cleaned.csv']\n",
      " Selected cluster ID: 23\n",
      "Logs sent to LLM:\n",
      " - userActivityNoUpdateLocked: eventTime=261973352, event=0, flags=0x0, uid=1000\n",
      "- userActivityNoUpdateLocked: eventTime=261977717, event=2, flags=0x0, uid=1000\n",
      "- userActivityNoUpdateLocked: eventTime=261973328, event=0, flags=0x0, uid=1000\n",
      "- userActivityNoUpdateLocked: eventTime=261949797, event=2, flags=0x0, uid=1000\n",
      "- userActivityNoUpdateLocked: eventTime=261973289, event=2, flags=0x0, uid=1000\n",
      "Semantic → \n",
      "\n",
      "The provided logs do not contain sufficient information to identify broadcast receiver triggers directly. However, based on the context of Android application logging, we can infer some possible relationships between the logs and broadcast receiver triggers.\n",
      "\n",
      "Broadcast receivers are components in Android that respond to system-wide broadcast events. They are registered to receive specific intents and are triggered when those intents are broadcast. The logs do not contain any explicit information about intents or broadcast events. However, we can look for patterns in the logs that might indicate broadcast receiver activity.\n",
      "\n",
      "One common pattern in logs related to broadcast receivers is the presence of `android.intent.action.BOOT_COMPLETED` intents, which are broadcast when the device finishes booting. In the given logs, we do not see any explicit mention of this intent. However, we do see repeated occurrences of the `userActivityNoUpdateLocked` event for user ID 1000.\n",
      "\n",
      "It is possible that this event represents the triggering of a broadcast receiver. The event name does not provide any clear indication of this, but based on the context of Android application logging, it is a reasonable assumption. Broadcast receivers are often used to update UI or perform other tasks in response to system events, such as user activity. The fact that this event is occurring multiple times for the same user ID suggests that it may be related to a broadcast receiver.\n",
      "\n",
      "To confirm this diagnosis, we would need to see more information in the logs, such as the names of the broadcast intents being received or the names of the broadcast receiver components. Without this information, it is difficult to make a definitive diagnosis based on the provided logs alone. However, the presence of repeated occurrences of the `userActivityNoUpdateLocked` event for the same user ID is a strong indication that broadcast receiver activity may be taking place.\n"
     ]
    }
   ],
   "source": [
    "query = \"Find broadcast receiver triggers in Android logs.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "150d0f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Retrieve broadcast receivers\n",
      "Inferred datasets: ['Android_2k_cleaned.csv', 'Hadoop_2k_cleaned.csv', 'HDFS_2k_cleaned.csv', 'Linux_2k_cleaned.csv', 'Openstack_2k_cleaned.csv']\n",
      " Selected cluster ID: 19\n",
      "Logs sent to LLM:\n",
      " - connection from 210.223.97.117 () at Wed Jun 29 14:44:35 2005\n",
      "- connection from 210.223.97.117 () at Wed Jun 29 14:44:35 2005\n",
      "- connection from 210.223.97.117 () at Wed Jun 29 14:44:35 2005\n",
      "- connection from 210.223.97.117 () at Wed Jun 29 14:44:35 2005\n",
      "- connection from 210.223.97.117 () at Wed Jun 29 14:44:35 2005\n",
      " - service startup patterns\n",
      "Inferred datasets: ['Android_2k_cleaned.csv', 'Hadoop_2k_cleaned.csv', 'HDFS_2k_cleaned.csv', 'Linux_2k_cleaned.csv', 'Openstack_2k_cleaned.csv']\n",
      " Selected cluster ID: 10\n",
      "Logs sent to LLM:\n",
      " - ready=true,policy=3,wakefulness=1,wksummary=0x0,uasummary=0x1,bootcompleted=true,boostinprogress=false,waitmodeenable=false,mode=false,manual=38,auto=-1,adj=0.0userId=0\n",
      "- ready=true,policy=3,wakefulness=1,wksummary=0x0,uasummary=0x1,bootcompleted=true,boostinprogress=false,waitmodeenable=false,mode=false,manual=38,auto=-1,adj=0.0userId=0\n",
      "- ready=true,policy=3,wakefulness=1,wksummary=0x0,uasummary=0x1,bootcompleted=true,boostinprogress=false,waitmodeenable=false,mode=false,manual=38,auto=-1,adj=0.0userId=0\n",
      "- ready=true,policy=3,wakefulness=1,wksummary=0x0,uasummary=0x1,bootcompleted=true,boostinprogress=false,waitmodeenable=false,mode=false,manual=38,auto=-1,adj=0.0userId=0\n",
      "- ready=true,policy=3,wakefulness=1,wksummary=0x0,uasummary=0x1,bootcompleted=true,boostinprogress=false,waitmodeenable=false,mode=false,manual=38,auto=-1,adj=0.0userId=0\n",
      "Semantic → \n",
      "\n",
      "Based on the provided logs, it appears that there is a single IP address (210.223.97.117) making multiple connections to the system at the same time. However, the user query is asking for \"broadcast receivers,\" which is a term typically used in the context of Android applications or other messaging systems.\n",
      "\n",
      "The logs do not provide enough context or information to definitively diagnose or answer the user query. It is possible that the IP address is a legitimate user making multiple connections for some reason, or it could be an attacker attempting to flood the system with connections. Without more information, it is impossible to determine if the IP address is related to broadcast receivers in any way.\n",
      "\n",
      "Therefore, based on the available logs, it is not possible to provide a clear and actionable answer to the user query. Further investigation and context would be required to make an accurate diagnosis or determination.\n",
      "Cluster → \n",
      "Based on the provided logs, there is a repeating pattern of a service starting up with the following characteristics:\n",
      "\n",
      "- The service is ready with a policy of 3.\n",
      "- The wakefulness is 1.\n",
      "- The work summary and user action summary are both 0x0.\n",
      "- The boot completed flag is true.\n",
      "- The boost in progress flag is false.\n",
      "- The wait mode enable flag is false.\n",
      "- The mode flag is false.\n",
      "- The manual flag is set to 38.\n",
      "- The auto flag is set to -1.\n",
      "- The adjustment value is 0.0.\n",
      "- The userId is 0.\n",
      "\n",
      "This pattern repeats multiple times in the logs. If you are looking for a more specific pattern or behavior, please provide more context or additional log data.\n"
     ]
    }
   ],
   "source": [
    "query = \"Retrieve broadcast receivers and service startup patterns\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be5ef26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - List requests to /servers/detail in osapi_compute handler\n",
      "Inferred datasets: ['Openstack_2k_cleaned.csv', 'Hadoop_2k_cleaned.csv']\n",
      " Selected cluster ID: 34\n",
      "Logs sent to LLM:\n",
      " - 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2551649\n",
      "- 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2556951\n",
      "- 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2656732\n",
      "- 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2640860\n",
      "- 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2661738\n",
      "Semantic → \n",
      "Based on the provided logs, there are five identical requests made to the /servers/detail endpoint in the osapi_compute handler. Each request resulted in a successful response (status code 200) and had the same response length (1893 bytes). Therefore, these requests likely represent a single user or process making multiple identical requests for the same data within a short time frame. This behavior could indicate inefficient coding or an attempt to bypass caching mechanisms. However, without further context or information, it is impossible to definitively diagnose the root cause of this behavior. It may be worth investigating potential performance optimizations or caching strategies to reduce the number of redundant requests.\n"
     ]
    }
   ],
   "source": [
    "query = \"List requests to /servers/detail in osapi_compute handler\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9287bc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Identify failures in YARN event dispatch or thread pool.\n",
      "Inferred datasets: ['Linux_2k_cleaned.csv', 'Hadoop_2k_cleaned.csv']\n",
      " Selected cluster ID: 37\n",
      " No logs in matching cluster, falling back to dataset-wide search\n",
      "Logs sent to LLM:\n",
      " - Thread Thread[eventHandlingThread,5,main] threw an Exception.\n",
      "- Event Writer setup for JobId: job_1445144423722_0020, File: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445144423722_0020/job_1445144423722_0020_1.jhist\n",
      "- yarn.client.max-cached-nodemanagers-proxies : 0\n",
      "- Kind: YARN_AM_RM_TOKEN, Service: , Ident: (appAttemptId { application_id { id: 20 cluster_timestamp: 1445144423722 } attemptId: 1 } keyId: -127633188)\n",
      "- Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445144423722_0020_01_000003 taskAttempt attempt_1445144423722_0020_m_000001_0\n",
      "Semantic → \n",
      "\n",
      "Based on the provided logs, there seems to be an exception thrown by a thread named \"eventHandlingThread\" in the YARN application manager. This exception could be indicative of a failure in YARN event dispatch or thread pool management.\n",
      "\n",
      "However, it is important to note that the logs do not directly indicate the cause of the exception. To further diagnose the issue, we would need to examine the stack trace or error message associated with the exception. Additionally, checking the YARN application logs for any related errors or warnings could provide more context.\n",
      "\n",
      "In the meantime, we can take some general actions to mitigate potential thread pool or event dispatch issues:\n",
      "\n",
      "1. Increase the number of cached nodemanagers proxies by setting the yarn.client.max-cached-nodemanagers-proxies property. This can help improve the availability of nodemanagers for container allocation and reduce the likelihood of thread pool starvation.\n",
      "\n",
      "2. Monitor the YARN application manager's thread pool usage and adjust the thread pool size as needed. This can be done by setting the yarn.scheduler.minimum-allocation-mb and yarn.scheduler.maximum-allocation-mb properties to control the minimum and maximum container sizes, respectively.\n",
      "\n",
      "3. Ensure that the HDFS namenode and secondary namenode are running properly and have sufficient resources to handle YARN event dispatch and container allocation requests.\n",
      "\n",
      "4. Check for any known issues or bugs related to YARN event dispatch or thread pool management in the Hadoop ecosystem. Upgrading to the latest stable version of Hadoop or applying relevant patches may help resolve the issue.\n"
     ]
    }
   ],
   "source": [
    "query = \"Identify failures in YARN event dispatch or thread pool.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bef0ee29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Identify broadcast intents related to BOOT_COMPLETED or activity launches\n",
      "Inferred datasets: ['Android_2k_cleaned.csv']\n",
      " Selected cluster ID: 16\n",
      "Logs sent to LLM:\n",
      " - Skipping, withExcluded: false, tr.intent:Intent { act=android.intent.action.MAIN flg=0x10840000 cmp=com.android.incallui/.InCallActivity (has extras) }\n",
      "- Skipping, withExcluded: false, tr.intent:Intent { flg=0x10804000 cmp=com.android.systemui/.recents.RecentsActivity bnds=[264,444][920,908] }\n",
      "- Skipping, withExcluded: false, tr.intent:Intent { flg=0x10804000 cmp=com.android.systemui/.recents.RecentsActivity bnds=[264,444][920,908] }\n",
      "- Skipping, withExcluded: false, tr.intent:Intent { flg=0x10804000 cmp=com.android.systemui/.recents.RecentsActivity bnds=[264,444][920,908] }\n",
      "- Skipping, withExcluded: false, tr.intent:Intent { flg=0x10804000 cmp=com.android.systemui/.recents.RecentsActivity bnds=[264,444][920,908] }\n",
      "Semantic → \n",
      "Based on the provided logs, there are no explicit broadcast intents related to BOOT_COMPLETED or activity launches mentioned. However, the logs do indicate several activity launches of the com.android.systemui.recents.RecentsActivity component. This component is known to be involved in the Android system UI for managing the recent applications list. While not directly related to the user query, it's worth noting that system components like com.android.systemui can broadcast intents in response to system events like BOOT_COMPLETED. To identify such intents, you would need to examine the system logs or use a more comprehensive log analysis tool that can capture and filter system broadcast intents.\n"
     ]
    }
   ],
   "source": [
    "query = \"Identify broadcast intents related to BOOT_COMPLETED or activity launches\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbbb5b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Detect unknown user login attempts from remote IPs.\n",
      "Inferred datasets: ['Linux_2k_cleaned.csv']\n",
      " Selected cluster ID: 41\n",
      "Logs sent to LLM:\n",
      " - authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root\n",
      "Semantic → \n",
      "Based on the provided logs, there are repeated authentication failure attempts originating from IP address 207.243.167.114. These attempts were made by the user \"root\" with a local user ID of 0. Since the same user and IP address are consistently appearing in the failure logs, it is likely that this is an unknown user attempting to gain unauthorized access to the system. It is recommended to investigate further and take appropriate security measures, such as blocking the IP address or implementing additional authentication measures.\n"
     ]
    }
   ],
   "source": [
    "query = \"Detect unknown user login attempts from remote IPs.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb6fabb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Show SSH login failures in Linux\n",
      "Inferred datasets: ['Linux_2k_cleaned.csv']\n",
      " Selected cluster ID: 41\n",
      "Logs sent to LLM:\n",
      " - authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root\n",
      " - PacketResponder terminations in HDFS.\n",
      "Inferred datasets: ['Hdfs_2k_cleaned.csv']\n",
      " Selected cluster ID: 27\n",
      " No logs in matching cluster, falling back to dataset-wide search\n",
      " No matching logs in any dataset — fallback failed\n",
      "Logs sent to LLM:\n",
      " \n",
      "Semantic → \n",
      "Based on the logs provided, there have been multiple failed SSH authentication attempts from the IP address 207.243.167.114. The user attempting to log in was the root user, with a UID and EUID of 0. This indicates that an unauthorized user is repeatedly trying to gain access to the system using SSH with the root account's credentials. It is highly recommended to investigate this further to determine the source of these attempts and take appropriate measures to secure the system, such as changing the root password, implementing two-factor authentication, or blocking the IP address.\n",
      "Semantic → \n",
      "The logs indicate that there have been terminations of PacketResponder processes in HDFS. This could be due to various reasons such as system failures, network issues, or application errors. To diagnose the root cause, it would be helpful to examine the logs leading up to the terminations for error messages or other indicators of the cause. Additionally, checking the HDFS cluster health and the status of other related components may provide context for the issue. If the terminations are frequent or recurring, further investigation may be necessary to prevent or mitigate the issue.\n",
      "\n",
      "Log Example:\n",
      "2022-03-15 10:30:21,345 [main] INFO org.apache.hadoop.hdfs.server.namenode.PacketResponder: PacketResponder handler 0.0.0.0:50070 terminated.\n",
      "2022-03-15 10:30:21,346 [main] INFO org.apache.hadoop.hdfs.server.namenode.PacketResponder: Starting new PacketResponder\n",
      "2022-03-15 10:30:21,346 [main] INFO org.apache.hadoop.hdfs.server.namenode.PacketResponder: PacketResponder handler 0.0.0.0:50070 started.\n",
      "\n",
      "Explanation:\n",
      "The logs show that there have been terminations and subsequent restarts of the PacketResponder process in HDFS. This process is responsible for handling client requests and responses in HDFS. Its termination could be due to various reasons such as system failures, network issues, or application errors. To diagnose the root cause, it would be helpful to examine the logs leading up to the terminations for error messages or other indicators of the cause. Additionally, checking the HDFS cluster health and the status of other related components may provide context for the issue. If the terminations are frequent or recurring, further investigation may be necessary to prevent or mitigate the issue.\n",
      "\n",
      "Context:\n",
      "HDFS (Hadoop Distributed File System) is a distributed file system used for storing and processing large data sets in Hadoop. The NameNode is the master node of the\n"
     ]
    }
   ],
   "source": [
    "query = \"Show SSH login failures in Linux and PacketResponder terminations in HDFS.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cc5f8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Correlate OpenStack instance launches with libvirt\n",
      "Inferred datasets: ['Openstack_2k_cleaned.csv']\n",
      " Selected cluster ID: 40\n",
      "Logs sent to LLM:\n",
      " - image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage\n",
      "- image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage\n",
      "- image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage\n",
      "- image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage\n",
      "- image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage\n",
      " - nova interactions.\n",
      "Inferred datasets: ['Openstack_2k_cleaned.csv']\n",
      " Selected cluster ID: 39\n",
      "Logs sent to LLM:\n",
      " - image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking\n",
      "- image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking\n",
      "- image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking\n",
      "- image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking\n",
      "- image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking\n",
      "Semantic → \n",
      "\n",
      "Based on the provided logs, it appears that there is an OpenStack instance (identified by the image ID 0673dd71-34c5-4fbb-86c4-40623fbe45b4) that is currently in use on a specific node. The logs indicate that the instance storage for this instance is being used locally on that node, with no usage reported on other nodes.\n",
      "\n",
      "The user query is asking for a correlation of OpenStack instance launches with libvirt. While the logs do not directly provide information on libvirt usage, they do indicate that the instance in question is being run on a node that uses OpenStack and libvirt for virtualization. Therefore, it can be inferred that the instance launch is correlated with libvirt usage on that node.\n",
      "\n",
      "However, it's important to note that the logs only show the current state of the instance and its storage usage. They do not provide information on when the instance was launched or how long it has been running. To get a more complete picture of the correlation between OpenStack instance launches and libvirt usage, additional logs or metrics would be needed.\n",
      "\n",
      "Recommendation:\n",
      "\n",
      "To get a more detailed view of the correlation between OpenStack instance launches and libvirt usage, consider using OpenStack's monitoring and logging features, such as Ceilometer and Logging Agent, to collect and analyze metrics and logs in real-time. This will provide more comprehensive data on instance launches, virtual machine instances, and libvirt usage, allowing for better analysis and troubleshooting. Additionally, tools like OpenStack Dashboard and OpenStack CLI can be used to query and visualize this data in various ways.\n",
      "Semantic → \n",
      "The logs indicate that the Nova compute service is performing checks on the image with ID 0673dd71-34c5-4fbb-86c4-40623fbe45b4. This image is likely being prepared for use in creating or updating a Nova instance. The repeated \"checking\" messages suggest that Nova is verifying the image's integrity and availability before proceeding. The user query does not provide enough context to determine if the interactions are related to a specific instance or operation, but the logs indicate that Nova is actively processing an image interaction.\n"
     ]
    }
   ],
   "source": [
    "query = \"Correlate OpenStack instance launches with libvirt and nova interactions.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37871118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - analyze recurring SSH authentication failures for the root user across multiple remote hosts in the Linux logs\n",
      "Inferred datasets: ['Linux_2k_cleaned.csv']\n",
      " Selected cluster ID: 41\n",
      "Logs sent to LLM:\n",
      " - authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root\n",
      " - especially those coming from unknown rhosts using NODEVssh\n",
      "Inferred datasets: ['Linux_2k_cleaned.csv']\n",
      " Selected cluster ID: 41\n",
      "Logs sent to LLM:\n",
      " - authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220.117.241.87  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220.117.241.87  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220.117.241.87  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220.117.241.87  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220.117.241.87  user=root\n",
      " - correlate with any denied sudo access attempts.\n",
      "Inferred datasets: ['Linux_2k_cleaned.csv']\n",
      " Selected cluster ID: 42\n",
      "Logs sent to LLM:\n",
      " - authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root\n",
      "- authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root\n",
      "Cluster → \n",
      "Based on the provided logs, there are repeated authentication failure events for the root user from the same remote IP address (207.243.167.114). This pattern indicates potential unauthorized access attempts to the system. Further investigation is required to determine the cause and potential remediation steps, such as reviewing firewall rules, checking system configurations, and enabling multi-factor authentication. Additionally, it may be beneficial to implement intrusion detection systems or security information and event management (SIEM) tools to monitor and alert on such patterns in real-time.\n",
      "Semantic → \n",
      "Based on the provided logs, there are repeated authentication failures originating from the IP address 220.117.241.87, with the same user (root) and the same SSH protocol (NODEVssh) being used. Although the logs do not explicitly state that the rhosts are \"unknown,\" the repetition of failed attempts from the same IP address raises suspicion. It is recommended to investigate the source of these attempts by performing the following actions:\n",
      "\n",
      "1. Check the firewall rules to ensure that unauthorized access from the IP address is being blocked.\n",
      "2. Review the system logs for any other relevant information, such as failed login attempts from other sources or suspicious system activity.\n",
      "3. Configure the SSH server to only allow access from trusted sources or to implement multi-factor authentication for added security.\n",
      "4. Consider implementing intrusion detection or prevention systems to monitor and block unauthorized access attempts.\n",
      "Semantic → \n",
      "Based on the provided logs, there are no denied sudo access attempts recorded in the logs. The logs only indicate multiple authentication failure attempts for the SSH service, where the user attempting to authenticate is 'root' from IP address '150.183.249.110'. While it's important to investigate the cause of these authentication failures, they do not directly correlate with any denied sudo access attempts as per the user query.\n"
     ]
    }
   ],
   "source": [
    "query = \"Identify and analyze recurring SSH authentication failures for the root user across multiple remote hosts in the Linux logs, especially those coming from unknown rhosts using NODEVssh, and correlate with any denied sudo access attempts.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0619e5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Correlate OpenStack instance launches with libvirt\n",
      "Inferred datasets: ['Openstack_2k_cleaned.csv']\n",
      " Selected cluster ID: 40\n",
      "Logs sent to LLM:\n",
      " - image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage\n",
      "- image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage\n",
      "- image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage\n",
      "- image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage\n",
      "- image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage\n",
      "Semantic → \n",
      "\n",
      "Based on the provided logs, it appears that the same OpenStack image (0673dd71-34c5-4fbb-86c4-40623fbe45b4) is being used to launch multiple instances on the same node. The logs indicate that the image is \"in use\" on the local node with no instances using it on other nodes. This suggests that each instance launch is using the same local image, which is being managed by OpenStack and libvirt on the local node.\n",
      "\n",
      "Therefore, the correlation between OpenStack instance launches and libvirt can be established based on the fact that libvirt is managing the local instances using the same image that OpenStack is providing. This relationship is further reinforced by the consistent message in each log entry, which indicates that the image is \"in use\" on the local node for each instance launch.\n",
      "\n",
      "Additional context:\n",
      "\n",
      "OpenStack Nova is the compute service in OpenStack that manages virtual machines and their instances. It uses libvirt as the hypervisor to manage the virtual machines. When an instance is launched, Nova requests an image from the OpenStack image service and provides it to libvirt for use in creating the virtual machine. The logs provided indicate that the same image is being used to launch multiple instances on the same node, which is consistent with the expected behavior of OpenStack and libvirt.\n",
      "\n",
      "To further diagnose or troubleshoot, it would be helpful to know if there are any issues with the image service or if there are any performance concerns with using the same image for multiple instances. Additionally, it may be worth investigating if there are any configuration issues that could be causing the instances to be launched on the same node repeatedly.\n"
     ]
    }
   ],
   "source": [
    "query = \"Correlate OpenStack instance launches with libvirt and nova interactions.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f46b6439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - Retrieve Android logs showing changes in fullscreen or docked stack bounds\n",
      "Logs sent to LLM:\n",
      " - setSystemUiVisibility vis=708 mask=ffffffff oldVal=40000600 newVal=708 diff=40000108 fullscreenStackVis=0 dockedStackVis=0, fullscreenStackBounds=Rect(0, 0 - 720, 1280), dockedStackBounds=Rect(0, 0 - 0, 0)\n",
      "- setSystemUiVisibility vis=708 mask=ffffffff oldVal=40000500 newVal=708 diff=40000208 fullscreenStackVis=0 dockedStackVis=0, fullscreenStackBounds=Rect(0, 0 - 720, 1280), dockedStackBounds=Rect(0, 0 - 0, 0)\n",
      "- setSystemUiVisibility vis=708 mask=ffffffff oldVal=40000500 newVal=708 diff=40000208 fullscreenStackVis=0 dockedStackVis=0, fullscreenStackBounds=Rect(0, 0 - 720, 1280), dockedStackBounds=Rect(0, 0 - 0, 0)\n",
      "- setSystemUiVisibility vis=508 mask=ffffffff oldVal=40000500 newVal=508 diff=40000008 fullscreenStackVis=0 dockedStackVis=0, fullscreenStackBounds=Rect(0, 0 - 720, 1280), dockedStackBounds=Rect(0, 0 - 0, 0)\n",
      "- setSystemUiVisibility vis=508 mask=ffffffff oldVal=40000500 newVal=508 diff=40000008 fullscreenStackVis=0 dockedStackVis=0, fullscreenStackBounds=Rect(0, 0 - 720, 1280), dockedStackBounds=Rect(0, 0 - 0, 0)\n",
      " - particularly where setSystemUiVisibility was invoked\n",
      "Logs sent to LLM:\n",
      " - setSystemUiVisibility vis=0 mask=1 oldVal=40000500 newVal=40000500 diff=0 fullscreenStackVis=0 dockedStackVis=0, fullscreenStackBounds=Rect(0, 0 - 0, 0), dockedStackBounds=Rect(0, 0 - 0, 0)\n",
      "- setSystemUiVisibility vis=0 mask=1 oldVal=40000500 newVal=40000500 diff=0 fullscreenStackVis=0 dockedStackVis=0, fullscreenStackBounds=Rect(0, 0 - 0, 0), dockedStackBounds=Rect(0, 0 - 0, 0)\n",
      "- setSystemUiVisibility vis=0 mask=1 oldVal=40000500 newVal=40000500 diff=0 fullscreenStackVis=0 dockedStackVis=0, fullscreenStackBounds=Rect(0, 0 - 0, 0), dockedStackBounds=Rect(0, 0 - 0, 0)\n",
      "- setSystemUiVisibility vis=0 mask=1 oldVal=40000500 newVal=40000500 diff=0 fullscreenStackVis=0 dockedStackVis=0, fullscreenStackBounds=Rect(0, 0 - 0, 0), dockedStackBounds=Rect(0, 0 - 0, 0)\n",
      "- setSystemUiVisibility vis=0 mask=1 oldVal=40000500 newVal=40000500 diff=0 fullscreenStackVis=0 dockedStackVis=0, fullscreenStackBounds=Rect(0, 0 - 0, 0), dockedStackBounds=Rect(0, 0 - 0, 0)\n",
      " - correlate with UI responsiveness metrics captured in policy flags.\n",
      "Logs sent to LLM:\n",
      " - ready=true,policy=3,wakefulness=1,wksummary=0x0,uasummary=0x1,bootcompleted=true,boostinprogress=false,waitmodeenable=false,mode=false,manual=38,auto=-1,adj=0.0userId=0\n",
      "- ready=true,policy=3,wakefulness=1,wksummary=0x0,uasummary=0x1,bootcompleted=true,boostinprogress=false,waitmodeenable=false,mode=false,manual=38,auto=-1,adj=0.0userId=0\n",
      "- ready=true,policy=3,wakefulness=1,wksummary=0x0,uasummary=0x1,bootcompleted=true,boostinprogress=false,waitmodeenable=false,mode=false,manual=38,auto=-1,adj=0.0userId=0\n",
      "- ready=true,policy=3,wakefulness=1,wksummary=0x0,uasummary=0x1,bootcompleted=true,boostinprogress=false,waitmodeenable=false,mode=false,manual=38,auto=-1,adj=0.0userId=0\n",
      "- ready=true,policy=3,wakefulness=1,wksummary=0x0,uasummary=0x1,bootcompleted=true,boostinprogress=false,waitmodeenable=false,mode=false,manual=38,auto=-1,adj=0.0userId=0\n",
      "Semantic → \n",
      "The provided logs show multiple instances of the \"setSystemUiVisibility\" system call being made with a new value of 708 for the \"vis\" parameter. This value corresponds to the \"SYSTEM_UI_FLAG_FULLSCREEN\" flag, which indicates that the system UI should be in fullscreen mode.\n",
      "\n",
      "The logs also include information about the fullscreen and docked stack bounds, which are specified in the \"fullscreenStackBounds\" and \"dockedStackBounds\" fields, respectively. These fields contain the dimensions of the areas on the screen where the fullscreen and docked stacks (i.e., the system UI and the app UI, respectively) are allowed to appear.\n",
      "\n",
      "Based on the information in the logs, it appears that there have been multiple instances where the system UI has been set to fullscreen mode (i.e., \"vis=708\") with no changes to the fullscreen or docked stack bounds. However, there is also one instance where the value of \"vis\" has been changed from 508 to 508, but the fullscreen and docked stack bounds have remained the same.\n",
      "\n",
      "Therefore, the logs do not show any changes to the fullscreen or docked stack bounds that correspond to the instances where the system UI has been set to fullscreen mode. It is possible that there are other logs or system events that may provide additional context about any changes to the stack bounds, but the provided logs alone do not contain sufficient information to make that determination.\n",
      "\n",
      "If the goal is to diagnose issues related to changes in fullscreen or docked stack bounds, it may be necessary to examine logs or system events that occur before and after the instances where the system UI is set to fullscreen mode, as well as any relevant configuration or application settings. Additionally, it may be helpful to consult the Android documentation and developer resources for more information about the \"setSystemUiVisibility\" system call and its relationship to fullscreen and docked stacks.\n",
      "Semantic → \n",
      "The logs indicate that the setSystemUiVisibility method was invoked five times with no changes to its parameters. Therefore, based on the context and the lack of variation in the logs, it appears that this method call is likely related to the initialization or configuration of the system UI, rather than a response to a user interaction or event.\n",
      "\n",
      "Additional context or reasoning:\n",
      "The setSystemUiVisibility method is a system-level API call used to modify the visibility and positioning of the system UI elements, such as the status bar, navigation bar, and system UI controls. The logs show that the method was called with the same parameters each time, indicating that the same configuration or initialization was being performed. This is a common pattern in Android app development, where certain system configurations or initializations are set up during app launch or initialization.\n",
      "\n",
      "Therefore, based on the context and the lack of variation in the logs, it appears that these method calls are likely related to the initialization or configuration of the system UI, rather than a response to a user interaction or event. This information could be useful for developers or system administrators who are trying to understand the behavior of an Android app or system.\n",
      "Semantic → \n",
      "\n",
      "Based on the provided logs, there is no correlation between the UI responsiveness metrics and the policy flags as all the logs show the same policy flag value (policy=3) and the same userId (userId=0). To diagnose the UI responsiveness issue, it would be necessary to examine other logs or metrics that might provide more context, such as CPU usage, memory usage, network latency, or user interactions. Additionally, it's important to note that the policy flags alone do not provide sufficient information to determine the root cause of UI responsiveness issues.\n"
     ]
    }
   ],
   "source": [
    "query = \"Retrieve Android logs showing changes in fullscreen or docked stack bounds, particularly where setSystemUiVisibility was invoked, and correlate with UI responsiveness metrics captured in policy flags.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2534d836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Subqueries from analyzer:\n",
      " - extract all job executions where resource allocation was throttled or denied by ResourceManager due to user-level limits or active container saturation\n",
      "Inferred datasets: ['Hadoop_2k_cleaned.csv']\n",
      " Selected cluster ID: 21\n",
      " No logs in matching cluster, falling back to dataset-wide search\n",
      "Logs sent to LLM:\n",
      " - Reduce slow start threshold not met. completedMapsForReduceSlowstart 1\n",
      "- Reduce slow start threshold not met. completedMapsForReduceSlowstart 1\n",
      "- Reduce slow start threshold not met. completedMapsForReduceSlowstart 1\n",
      "- Reduce slow start threshold not met. completedMapsForReduceSlowstart 1\n",
      "- Reduce slow start threshold not met. completedMapsForReduceSlowstart 1\n",
      " - indicate the affected user/application ID.\n",
      "Inferred datasets: ['Android_2k_cleaned.csv', 'Hadoop_2k_cleaned.csv', 'HDFS_2k_cleaned.csv', 'Linux_2k_cleaned.csv', 'Openstack_2k_cleaned.csv']\n",
      " Selected cluster ID: 17\n",
      "Logs sent to LLM:\n",
      " - check pass; user unknown\n",
      "- check pass; user unknown\n",
      "- check pass; user unknown\n",
      "- check pass; user unknown\n",
      "- check pass; user unknown\n",
      "Semantic → \n",
      "\n",
      "Based on the provided logs, it appears that a MapReduce job has encountered the \"Reduce slow start threshold not met\" issue multiple times. This issue is related to the MapReduce task initialization process, where the number of completed mappers does not meet the slow start threshold within the specified time frame. This issue does not directly indicate resource allocation being throttled or denied due to user-level limits or active container saturation.\n",
      "\n",
      "However, it's important to note that the \"Reduce slow start threshold not met\" issue can indirectly be a symptom of resource contention or saturation within the cluster. If the cluster is experiencing high resource usage, it may lead to slower MapReduce task initialization times, causing the \"Reduce slow start threshold not met\" issue. In such cases, it's recommended to investigate the overall resource usage and capacity of the cluster to determine if resource allocation is being throttled or denied due to user-level limits or active container saturation.\n",
      "\n",
      "To diagnose the issue further, you can check the following:\n",
      "\n",
      "1. Resource usage and capacity of the cluster: Check the Hadoop cluster's resource usage and capacity by examining the YARN Resource Manager's logs, metrics, and web UI. Look for signs of high resource usage, such as high CPU or memory usage, and check if there are any active users or applications consuming a disproportionate amount of resources.\n",
      "\n",
      "2. Job and task details: Examine the MapReduce job and task details, such as the job's input and output sizes, the number of mappers and reducers, and the amount of memory allocated to each task. Check if the job's resource requirements are within the cluster's capacity and if the tasks are completing within a reasonable time frame.\n",
      "\n",
      "3. User-level limits: Check the Hadoop configuration settings for user-level limits, such as the `yarn.scheduler.minimum-allocation` and `yarn.scheduler.maximum-allocation` properties. Ensure that the user-level limits are set appropriately and are not causing the job to be throttled or denied resources.\n",
      "\n",
      "4. Container saturation: Check if there is any container saturation within the cluster by examining the YARN Resource Manager's logs and metrics. Look for signs of high container contention, such as long container queues or containers\n",
      "Semantic → \n",
      "Based on the provided logs, there is no clear indication of an affected user or application ID. All the logs show the same event \"check pass\" with an unknown user. To determine the affected user or application ID, more context or information is required, such as the source IP address, timestamp, or application name associated with each log entry.\n"
     ]
    }
   ],
   "source": [
    "query = \"From Hadoop logs, extract all job executions where resource allocation was throttled or denied by ResourceManager due to user-level limits or active container saturation, and indicate the affected user/application ID.\"\n",
    "response = llm_agent.run_llm_agent(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff37ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d46d981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b729776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b69d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444e1ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9481fd54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616cda95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e87c822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6319d7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import sys\n",
    "sys.path.append(\"/workspace/FCAPS/RAG\")\n",
    "# from rag_fxn import search_semantic\n",
    "from multilevel_rag_fxn import search_semantic\n",
    "\n",
    "\n",
    "semantic_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c311bf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: [('Android_2k_cleaned.csv', 233, 0.0, np.float32(-9.626), 'Skipping, withExcluded: false, tr.intent:Intent { act=android.intent.action.MAIN flg=0x10840000 cmp=com.android.incallui/.InCallActivity (has extras) }'), ('Android_2k_cleaned.csv', 232, 0.0, np.float32(-9.9487), 'Skipping, withExcluded: false, tr.intent:Intent { flg=0x10804000 cmp=com.android.systemui/.recents.RecentsActivity bnds=[264,444][920,908] }'), ('Android_2k_cleaned.csv', 232, 0.0, np.float32(-10.1986), 'Skipping, withExcluded: false, tr.intent:Intent { flg=0x10804000 cmp=com.android.systemui/.recents.RecentsActivity bnds=[264,444][920,908] }'), ('Android_2k_cleaned.csv', 233, 0.0, np.float32(-10.4257), 'Skipping, withExcluded: false, tr.intent:Intent { act=android.intent.action.MAIN flg=0x10840000 cmp=com.android.incallui/.InCallActivity (has extras) }')]\n"
     ]
    }
   ],
   "source": [
    "query = \"Identify activity launch patterns in the logs of android.\"\n",
    "\n",
    "results = search_semantic(query=query, model=semantic_model, top_k=5)\n",
    "print(\"Results:\", results)\n",
    "\n",
    "if not results:\n",
    "    print(\"No relevant logs found.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9d82a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Android_2k_cleaned.csv', 232, 0.0, np.float32(-8.7694), 'Skipping, withExcluded: false, tr.intent:Intent { flg=0x10804000 cmp=com.android.systemui/.recents.RecentsActivity bnds=[264,444][920,908] }'), ('Android_2k_cleaned.csv', 234, 0.0, np.float32(-9.7238), 'Skipping, withExcluded: false, tr.intent:Intent { flg=0x18800000 cmp=com.tencent.mm/.plugin.base.stub.WXEntryActivity (has extras) }'), ('Android_2k_cleaned.csv', 232, 0.0, np.float32(-8.7077), 'Skipping, withExcluded: false, tr.intent:Intent { flg=0x10804000 cmp=com.android.systemui/.recents.RecentsActivity bnds=[264,444][920,908] }'), ('Android_2k_cleaned.csv', 234, 0.0, np.float32(-9.9301), 'Skipping, withExcluded: false, tr.intent:Intent { flg=0x18800000 cmp=com.tencent.mm/.plugin.base.stub.WXEntryActivity (has extras) }')]\n"
     ]
    }
   ],
   "source": [
    "query = \"Identify log entries where intents were fired for launching system UI. \"\n",
    "\n",
    "results = search_semantic(query=query, model=semantic_model, top_k=5)\n",
    "print(results)\n",
    "\n",
    "if not results:\n",
    "    print(\"No relevant logs found.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caac876b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred datasets: ['Android_2k_cleaned.csv']\n",
      " Selected cluster ID: 23\n",
      "Results: [('Android_2k_cleaned.csv', 1412, -10.9548, 'userActivityNoUpdateLocked: eventTime=261973352, event=0, flags=0x0, uid=1000'), ('Android_2k_cleaned.csv', 1402, -10.9622, 'userActivityNoUpdateLocked: eventTime=261973328, event=0, flags=0x0, uid=1000'), ('Android_2k_cleaned.csv', 1601, -10.9699, 'userActivityNoUpdateLocked: eventTime=261977717, event=2, flags=0x0, uid=1000'), ('Android_2k_cleaned.csv', 176, -10.987, 'userActivityNoUpdateLocked: eventTime=261850777, event=2, flags=0x0, uid=1000'), ('Android_2k_cleaned.csv', 1597, -10.9891, 'userActivityNoUpdateLocked: eventTime=261976915, event=2, flags=0x0, uid=1000')]\n"
     ]
    }
   ],
   "source": [
    "query = \"Show all logs related to RecentsActivity or app switching behavior.\"\n",
    "\n",
    "results = search_semantic(query=query, model=semantic_model, top_k=5)\n",
    "print(\"Results:\", results)\n",
    "\n",
    "if not results:\n",
    "    print(\"No relevant logs found.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5c3eed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred datasets: ['Android_2k_cleaned.csv']\n",
      " Selected cluster ID: 16\n",
      "Results: [('Android_2k_cleaned.csv', 1870, -10.0239, 'Skipping, withExcluded: false, tr.intent:Intent { flg=0x10804000 cmp=com.android.systemui/.recents.RecentsActivity bnds=[264,444][920,908] }'), ('Android_2k_cleaned.csv', 1090, -10.1598, 'Skipping, withExcluded: false, tr.intent:Intent { act=android.intent.action.MAIN flg=0x10840000 cmp=com.android.incallui/.InCallActivity (has extras) }'), ('Android_2k_cleaned.csv', 1366, -10.1598, 'Skipping, withExcluded: false, tr.intent:Intent { act=android.intent.action.MAIN flg=0x10840000 cmp=com.android.incallui/.InCallActivity (has extras) }'), ('Android_2k_cleaned.csv', 1513, -10.1598, 'Skipping, withExcluded: false, tr.intent:Intent { act=android.intent.action.MAIN flg=0x10840000 cmp=com.android.incallui/.InCallActivity (has extras) }'), ('Android_2k_cleaned.csv', 1370, -10.1598, 'Skipping, withExcluded: false, tr.intent:Intent { act=android.intent.action.MAIN flg=0x10840000 cmp=com.android.incallui/.InCallActivity (has extras) }')]\n"
     ]
    }
   ],
   "source": [
    "query = \"Detect any broadcast receiver triggers or service launches in Android.\"\n",
    "\n",
    "results = search_semantic(query=query, model=semantic_model, top_k=5)\n",
    "print(\"Results:\", results)\n",
    "\n",
    "if not results:\n",
    "    print(\"No relevant logs found.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0052788b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred datasets: ['Linux_2k_cleaned.csv']\n",
      " Selected cluster ID: 41\n",
      "Results: [('Linux_2k_cleaned.csv', 1829, 1.1915, 'authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root'), ('Linux_2k_cleaned.csv', 1833, 1.1915, 'authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root'), ('Linux_2k_cleaned.csv', 1825, 1.1915, 'authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root'), ('Linux_2k_cleaned.csv', 1828, 1.1915, 'authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root'), ('Linux_2k_cleaned.csv', 1834, 1.1915, 'authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root')]\n"
     ]
    }
   ],
   "source": [
    "query = \"Find logs related to SSH authentication failures.\"\n",
    "\n",
    "results = search_semantic(query=query, model=semantic_model, top_k=5)\n",
    "print(\"Results:\", results)\n",
    "\n",
    "if not results:\n",
    "    print(\"No relevant logs found.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "542b97d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred datasets: ['Linux_2k_cleaned.csv']\n",
      " Selected cluster ID: 42\n",
      "Results: [('Linux_2k_cleaned.csv', 1167, -9.5354, 'authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root'), ('Linux_2k_cleaned.csv', 1098, -9.5354, 'authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root'), ('Linux_2k_cleaned.csv', 1121, -9.5354, 'authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root'), ('Linux_2k_cleaned.csv', 1106, -9.5354, 'authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root'), ('Linux_2k_cleaned.csv', 1145, -9.5354, 'authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root')]\n"
     ]
    }
   ],
   "source": [
    "query = \"Identify unauthorized login attempts or access denials.\"\n",
    "\n",
    "results = search_semantic(query=query, model=semantic_model, top_k=5)\n",
    "print(\"Results:\", results)\n",
    "\n",
    "if not results:\n",
    "    print(\"No relevant logs found.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5afc592f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred datasets: ['Linux_2k_cleaned.csv']\n",
      " Selected cluster ID: 42\n",
      "Results: [('Linux_2k_cleaned.csv', 1167, -8.8716, 'authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root'), ('Linux_2k_cleaned.csv', 1098, -8.8716, 'authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root'), ('Linux_2k_cleaned.csv', 1121, -8.8716, 'authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root'), ('Linux_2k_cleaned.csv', 1106, -8.8716, 'authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root'), ('Linux_2k_cleaned.csv', 1145, -8.8716, 'authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root')]\n"
     ]
    }
   ],
   "source": [
    "query = \"Show logs where root or sudo access was attempted.\"\n",
    "\n",
    "results = search_semantic(query=query, model=semantic_model, top_k=5)\n",
    "print(\"Results:\", results)\n",
    "\n",
    "if not results:\n",
    "    print(\"No relevant logs found.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "910bf87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred datasets: ['Linux_2k_cleaned.csv']\n",
      " Selected cluster ID: 3\n",
      "Results: [('Linux_2k_cleaned.csv', 579, -5.6455, 'session closed for user test'), ('Linux_2k_cleaned.csv', 89, -5.6455, 'session closed for user test'), ('Linux_2k_cleaned.csv', 576, -5.6455, 'session closed for user test'), ('Linux_2k_cleaned.csv', 597, -5.6455, 'session closed for user test'), ('Linux_2k_cleaned.csv', 647, -5.6455, 'session closed for user test')]\n"
     ]
    }
   ],
   "source": [
    "query = \"Analyze user login sessions from different terminals.\"\n",
    "\n",
    "results = search_semantic(query=query, model=semantic_model, top_k=5)\n",
    "print(\"Results:\", results)\n",
    "\n",
    "if not results:\n",
    "    print(\"No relevant logs found.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85a5e74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred datasets: ['Openstack_2k_cleaned.csv']\n",
      " Selected cluster ID: 35\n",
      "Results: [('Openstack_2k_cleaned.csv', 493, -2.488, '10.11.21.127,10.11.10.1 \"GET /openstack/2013-10-17 HTTP/1.1\" status: 200 len: 157 time: 0.2243600'), ('Openstack_2k_cleaned.csv', 859, -2.5174, '10.11.21.131,10.11.10.1 \"GET /openstack/2013-10-17/vendor_data.json HTTP/1.1\" status: 200 len: 124 time: 0.2301619'), ('Openstack_2k_cleaned.csv', 1517, -2.636, '10.11.21.138,10.11.10.1 \"GET /openstack/2013-10-17/vendor_data.json HTTP/1.1\" status: 200 len: 124 time: 0.2222931'), ('Openstack_2k_cleaned.csv', 48, -2.6987, '10.11.21.122,10.11.10.1 \"GET /openstack/2013-10-17/vendor_data.json HTTP/1.1\" status: 200 len: 124 time: 0.2197890'), ('Openstack_2k_cleaned.csv', 1334, -2.7423, '10.11.21.136,10.11.10.1 \"GET /openstack/2013-10-17/vendor_data.json HTTP/1.1\" status: 200 len: 124 time: 0.2191730')]\n"
     ]
    }
   ],
   "source": [
    "query = \"Show HTTP API errors in OpenStack..\"\n",
    "\n",
    "results = search_semantic(query=query, model=semantic_model, top_k=5)\n",
    "print(\"Results:\", results)\n",
    "\n",
    "if not results:\n",
    "    print(\"No relevant logs found.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aff465a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred datasets: ['Openstack_2k_cleaned.csv']\n",
      " Selected cluster ID: 37\n",
      "Results: [('Openstack_2k_cleaned.csv', 364, -11.1331, '[instance: 95960536-049b-41f6-9049-05fc479b6a7c] VM Started (Lifecycle Event)'), ('Openstack_2k_cleaned.csv', 1575, -11.1361, '[instance: d96a117b-0193-4549-bdcc-63b917273d1d] VM Started (Lifecycle Event)'), ('Openstack_2k_cleaned.csv', 446, -11.1549, '[instance: 95960536-049b-41f6-9049-05fc479b6a7c] VM Stopped (Lifecycle Event)'), ('Openstack_2k_cleaned.csv', 546, -11.171, '[instance: af5f7392-f7d4-4298-b647-c98924c64aa1] VM Started (Lifecycle Event)'), ('Openstack_2k_cleaned.csv', 1678, -11.1802, '[instance: d6b7bd36-2943-4363-9235-fffdd89ea40e] VM Started (Lifecycle Event)')]\n"
     ]
    }
   ],
   "source": [
    "query = \" Identify failed virtual machine creation attempts.\"\n",
    "\n",
    "results = search_semantic(query=query, model=semantic_model, top_k=5)\n",
    "print(\"Results:\", results)\n",
    "\n",
    "if not results:\n",
    "    print(\"No relevant logs found.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c0cd040",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred datasets: ['Openstack_2k_cleaned.csv']\n",
      " Selected cluster ID: 35\n",
      "Results: [('Openstack_2k_cleaned.csv', 1517, 0.1068, '10.11.21.138,10.11.10.1 \"GET /openstack/2013-10-17/vendor_data.json HTTP/1.1\" status: 200 len: 124 time: 0.2222931'), ('Openstack_2k_cleaned.csv', 1604, 0.0357, '10.11.21.139,10.11.10.1 \"GET /openstack/2013-10-17/vendor_data.json HTTP/1.1\" status: 200 len: 124 time: 0.0006561'), ('Openstack_2k_cleaned.csv', 1510, -0.0035, '10.11.21.138,10.11.10.1 \"GET /openstack/2013-10-17/vendor_data.json HTTP/1.1\" status: 200 len: 124 time: 0.2357898'), ('Openstack_2k_cleaned.csv', 1427, -0.0699, '10.11.21.137,10.11.10.1 \"GET /openstack/2013-10-17/vendor_data.json HTTP/1.1\" status: 200 len: 124 time: 0.0005460'), ('Openstack_2k_cleaned.csv', 399, -0.105, '10.11.21.126,10.11.10.1 \"GET /openstack/2013-10-17/vendor_data.json HTTP/1.1\" status: 200 len: 124 time: 0.2317870')]\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the common status codes in OpenStack API logs?\"\n",
    "\n",
    "results = search_semantic(query=query, model=semantic_model, top_k=5)\n",
    "print(\"Results:\", results)\n",
    "\n",
    "if not results:\n",
    "    print(\"No relevant logs found.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f6fb8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred datasets: ['Linux_2k_cleaned.csv', 'Openstack_2k_cleaned.csv']\n",
      " Selected cluster ID: 35\n",
      "Results: [('Openstack_2k_cleaned.csv', 585, -4.623, '10.11.21.128,10.11.10.1 \"GET /openstack/2013-10-17/vendor_data.json HTTP/1.1\" status: 200 len: 124 time: 0.2283430'), ('Openstack_2k_cleaned.csv', 1517, -4.6817, '10.11.21.138,10.11.10.1 \"GET /openstack/2013-10-17/vendor_data.json HTTP/1.1\" status: 200 len: 124 time: 0.2222931'), ('Openstack_2k_cleaned.csv', 1334, -4.6867, '10.11.21.136,10.11.10.1 \"GET /openstack/2013-10-17/vendor_data.json HTTP/1.1\" status: 200 len: 124 time: 0.2191730'), ('Openstack_2k_cleaned.csv', 676, -4.7316, '10.11.21.129,10.11.10.1 \"GET /openstack/2013-10-17/vendor_data.json HTTP/1.1\" status: 200 len: 124 time: 0.2354860'), ('Openstack_2k_cleaned.csv', 768, -4.7398, '10.11.21.130,10.11.10.1 \"GET /openstack/2013-10-17/vendor_data.json HTTP/1.1\" status: 200 len: 124 time: 0.2199168')]\n"
     ]
    }
   ],
   "source": [
    "query = \"Detect storage-related API failures or timeouts in openstack.\"\n",
    "\n",
    "results = search_semantic(query=query, model=semantic_model, top_k=5)\n",
    "print(\"Results:\", results)\n",
    "\n",
    "if not results:\n",
    "    print(\"No relevant logs found.\")\n",
    "    exit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8f0767d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred datasets: ['Hadoop_2k_cleaned.csv']\n",
      " Selected cluster ID: 25\n",
      " No logs in matching cluster, falling back to dataset-wide search\n",
      "Results: [('Hadoop_2k_cleaned.csv', 1023, -4.2315, 'Error writing History Event: org.apache.hadoop.mapreduce.jobhistory.TaskAttemptUnsuccessfulCompletionEvent@7317849d'), ('Hadoop_2k_cleaned.csv', 12, -5.1132, 'Registering class org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher'), ('Hadoop_2k_cleaned.csv', 13, -5.2005, 'Registering class org.apache.hadoop.mapreduce.v2.app.job.event.TaskEventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher'), ('Hadoop_2k_cleaned.csv', 8, -5.2285, 'Registering class org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncher$EventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerLauncherRouter'), ('Hadoop_2k_cleaned.csv', 15, -5.432, 'Registering class org.apache.hadoop.mapreduce.jobhistory.EventType for class org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler')]\n"
     ]
    }
   ],
   "source": [
    "query = \"Find hadoop logs with ResourceManager events or heartbeats.\"\n",
    "\n",
    "results = search_semantic(query=query, model=semantic_model, top_k=5)\n",
    "print(\"Results:\", results)\n",
    "\n",
    "if not results:\n",
    "    print(\"No relevant logs found.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4c10f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred datasets: ['Hadoop_2k_cleaned.csv']\n",
      " Selected cluster ID: 18\n",
      "Results: [('Hadoop_2k_cleaned.csv', 1885, -11.3946, 'Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 324 seconds.  Will retry shortly ...'), ('Hadoop_2k_cleaned.csv', 1978, -11.3947, 'Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 351 seconds.  Will retry shortly ...'), ('Hadoop_2k_cleaned.csv', 1353, -11.3968, 'Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 173 seconds.  Will retry shortly ...'), ('Hadoop_2k_cleaned.csv', 1991, -11.3971, 'Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 354 seconds.  Will retry shortly ...'), ('Hadoop_2k_cleaned.csv', 1846, -11.3971, 'Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 313 seconds.  Will retry shortly ...')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"Identify issues with job submission or callQueue overflows in hadoop.\"\n",
    "\n",
    "results = search_semantic(query=query, model=semantic_model, top_k=5)\n",
    "print(\"Results:\", results)\n",
    "\n",
    "if not results:\n",
    "    print(\"No relevant logs found.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8906e5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred datasets: ['Linux_2k_cleaned.csv', 'Hadoop_2k_cleaned.csv']\n",
      " Selected cluster ID: 37\n",
      " No logs in matching cluster, falling back to dataset-wide search\n",
      "Results: [('Hadoop_2k_cleaned.csv', 1023, -3.2102, 'Error writing History Event: org.apache.hadoop.mapreduce.jobhistory.TaskAttemptUnsuccessfulCompletionEvent@7317849d'), ('Hadoop_2k_cleaned.csv', 135, -4.0632, 'The job-conf file on the remote FS is /tmp/hadoop-yarn/staging/msrabi/.staging/job_1445144423722_0020/job.xml'), ('Hadoop_2k_cleaned.csv', 15, -4.3109, 'Registering class org.apache.hadoop.mapreduce.jobhistory.EventType for class org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler'), ('Hadoop_2k_cleaned.csv', 14, -4.9061, 'Registering class org.apache.hadoop.mapreduce.v2.app.job.event.JobEventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher'), ('Hadoop_2k_cleaned.csv', 136, -4.9132, 'The job-jar file on the remote FS is hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445144423722_0020/job.jar')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"List hadoop logs that mention job creation or scheduling failures.\"\n",
    "\n",
    "results = search_semantic(query=query, model=semantic_model, top_k=5)\n",
    "print(\"Results:\", results)\n",
    "\n",
    "if not results:\n",
    "    print(\"No relevant logs found.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab04a400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred datasets: ['Linux_2k_cleaned.csv', 'Hdfs_2k_cleaned.csv']\n",
      " Selected cluster ID: 18\n",
      " No logs in matching cluster, falling back to dataset-wide search\n",
      "Results: [('Linux_2k_cleaned.csv', 1423, -10.7921, 'connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:02:59 2005'), ('Linux_2k_cleaned.csv', 1422, -10.7921, 'connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:02:59 2005'), ('Linux_2k_cleaned.csv', 1424, -10.7992, 'connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:03:00 2005'), ('Linux_2k_cleaned.csv', 1425, -10.7992, 'connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:03:00 2005'), ('Linux_2k_cleaned.csv', 1426, -10.7992, 'connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:03:00 2005')]\n"
     ]
    }
   ],
   "source": [
    "query = \"List logs that mention job creation or scheduling failures in hdfs.\"\n",
    "\n",
    "results = search_semantic(query=query, model=semantic_model, top_k=5)\n",
    "print(\"Results:\", results)\n",
    "\n",
    "if not results:\n",
    "    print(\"No relevant logs found.\")\n",
    "    exit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a44b27a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred datasets: ['Hdfs_2k_cleaned.csv']\n",
      " Selected cluster ID: 25\n",
      " No logs in matching cluster, falling back to dataset-wide search\n",
      " No matching logs in any dataset — fallback failed\n",
      "Results: []\n",
      "No relevant logs found.\n"
     ]
    }
   ],
   "source": [
    "query = \"Identify HDFS storage directory issues.\"\n",
    "\n",
    "results = search_semantic(query=query, model=semantic_model, top_k=5)\n",
    "print(\"Results:\", results)\n",
    "\n",
    "if not results:\n",
    "    print(\"No relevant logs found.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de4962fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred datasets: ['Hdfs_2k_cleaned.csv']\n",
      " Selected cluster ID: 32\n",
      " No logs in matching cluster, falling back to dataset-wide search\n",
      " No matching logs in any dataset — fallback failed\n",
      "Results: []\n",
      "No relevant logs found.\n"
     ]
    }
   ],
   "source": [
    "query = \"Find logs where blocks are received or terminated..\"\n",
    "\n",
    "results = search_semantic(query=query, model=semantic_model, top_k=5)\n",
    "print(\"Results:\", results)\n",
    "\n",
    "if not results:\n",
    "    print(\"No relevant logs found.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a1cce40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS Index: 9849 vectors loaded.\n",
      "Metadata: 9849 rows loaded.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Paths to saved files\n",
    "index_path = \"/workspace/FCAPS/RAG/logs.index\"\n",
    "metadata_path = \"/workspace/FCAPS/RAG/metadata_with_clusters.csv\"\n",
    "\n",
    "# Load the FAISS index\n",
    "index = faiss.read_index(index_path)\n",
    "\n",
    "# Load metadata\n",
    "metadata_df = pd.read_csv(metadata_path)\n",
    "\n",
    "# Verify index and metadata\n",
    "print(f\"FAISS Index: {index.ntotal} vectors loaded.\")\n",
    "print(f\"Metadata: {metadata_df.shape[0]} rows loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eef36dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def run_mistral(query, results):  # 🛑 Only added this wrapper line\n",
    "    prompt = \"\"\"\n",
    "[INST]Given a dataset of Linux logs, we aim to detect a higher concentration of authentication failures and identify their patterns. We have retrieved multiple log entries where authentication failures occurred, all originating from the same IP address (163.27.187.39).\n",
    "\n",
    "Step-by-Step Analysis:\n",
    "1.Extract Key Information: Identify repeated patterns in authentication failure logs.\n",
    "2. Detect Anomalies: Determine if failures originate from a single IP or multiple sources.\n",
    "3. Analyze Frequency: Assess whether failures occur in bursts or continuously over time.\n",
    "4. Identify Possible Causes: Consider reasons such as brute-force attacks, expired credentials, or network issues.\n",
    "5. Suggest Actions: Recommend security measures, such as blocking the IP, enabling CAPTCHA, or monitoring failed attempts.\n",
    "Task:\n",
    "Based on the given authentication failure logs, analyze the pattern of failures and explain whether this suggests an attack or a misconfiguration. Provide reasoning for your conclusion.[/INST]\n",
    "    \"\"\"\n",
    "\n",
    "    for dataset, log_index, similarity, log_content in results:\n",
    "        prompt += f\"- **Dataset**: {dataset}, **Log Index**: {log_index}, **Similarity**: {similarity:.4f}\\n\"\n",
    "        prompt += f\"  **Log Content**: {log_content}\\n\\n\"\n",
    "\n",
    "    prompt += \"[/INST]\"\n",
    "\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=2048, num_return_sequences=1, do_sample=True, temperature=1.2)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    response_path = \"/workspace/FCAPS/responses/response.json\"\n",
    "    markdown_path = \"/workspace/FCAPS/responses/response.md\"\n",
    "\n",
    "    if os.path.exists(response_path):\n",
    "        with open(response_path, \"r\") as f:\n",
    "            previous_responses = json.load(f)\n",
    "    else:\n",
    "        previous_responses = []\n",
    "\n",
    "    # Append new response to the list\n",
    "    response_data = {\n",
    "        \"query\": query,\n",
    "        \"response\": response,\n",
    "        \"logs\": [\n",
    "            {\n",
    "                \"dataset\": dataset,\n",
    "                \"log_index\": int(log_index),\n",
    "                \"similarity\": float(similarity),\n",
    "                \"log_content\": log_content\n",
    "            }\n",
    "            for dataset, log_index, similarity, log_content in results\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    previous_responses.append(response_data)\n",
    "\n",
    "    with open(response_path, \"w\") as f:\n",
    "        json.dump(previous_responses, f, indent=4)\n",
    "\n",
    "    with open(markdown_path, \"a\") as f:  \n",
    "        f.write(f\"\\n\\n---\\n# Response for Query: {query}\\n\")\n",
    "        f.write(f\"### Logs Used:\\n\")\n",
    "        for entry in response_data[\"logs\"]:\n",
    "            f.write(f\"- Dataset: {entry['dataset']}, Log Index: {entry['log_index']}, Similarity: {entry['similarity']:.4f}\\n\")\n",
    "            f.write(f\"  **Log Content**: {entry['log_content']}\\n\\n\")\n",
    "        f.write(f\"\\n### Model Response:\\n{response}\\n\")\n",
    "\n",
    "    print(f\"Response saved successfully!\\n- JSON: {response_path}\\n- Markdown: {markdown_path}\")\n",
    "\n",
    "    return response  # 🛑 Added so server can get response back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "056452c0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel is on device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput tensor is on device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Model is on device: {model.device}\")\n",
    "print(f\"Input tensor is on device: {input_ids.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5461707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (prism_env)",
   "language": "python",
   "name": "prism_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
