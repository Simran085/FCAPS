{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1abb0009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Android_2k_cleaned.csv: 2000 logs\n",
      "Processed Hadoop_2k_cleaned.csv: 2000 logs\n",
      "Processed HDFS_2k_cleaned.csv: 2000 logs\n",
      "Processed Linux_2k_cleaned.csv: 1849 logs\n",
      "Processed Openstack_2k_cleaned.csv: 2000 logs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/FCAPS/prism_env/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/workspace/FCAPS/prism_env/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster centroids computed and saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Multi-Level RAG with dataset-aware routing\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import hdbscan\n",
    "import pickle\n",
    "\n",
    "DATASET_FOLDER = \"/workspace/FCAPS/dataset/Cleaned datasets\"\n",
    "RAG_FOLDER = \"/workspace/FCAPS/RAG\"\n",
    "os.makedirs(RAG_FOLDER, exist_ok=True)\n",
    "\n",
    "# Load embedding model\n",
    "def select_embedding_model(model_name):\n",
    "    return SentenceTransformer(f\"sentence-transformers/{model_name}\")\n",
    "\n",
    "model = select_embedding_model(\"all-MiniLM-L6-v2\")\n",
    "embedding_dim = 384\n",
    "index = faiss.IndexHNSWFlat(embedding_dim, 32)\n",
    "\n",
    "metadata = []\n",
    "embeddings_all = []\n",
    "log_mapping = {}\n",
    "\n",
    "# Dataset-level encoding\n",
    "label_encoder = LabelEncoder()\n",
    "dataset_names = [f for f in os.listdir(DATASET_FOLDER) if f.endswith(\".csv\")]\n",
    "dataset_labels = label_encoder.fit_transform(dataset_names)\n",
    "dataset_name_to_label = dict(zip(dataset_names, dataset_labels))\n",
    "\n",
    "start_idx = 0\n",
    "\n",
    "# Ingest logs and encode \n",
    "for file in dataset_names:\n",
    "    dataset_path = os.path.join(DATASET_FOLDER, file)\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    log_texts = df['Content'].astype(str).tolist()\n",
    "\n",
    "    embeddings = model.encode(log_texts, convert_to_numpy=True)\n",
    "    embeddings /= np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "    index.add(embeddings)\n",
    "    embeddings_all.append(embeddings)\n",
    "\n",
    "    for i, log in enumerate(log_texts):\n",
    "        faiss_idx = start_idx + i\n",
    "        log_mapping[faiss_idx] = (file, i, log)\n",
    "        metadata.append((file, faiss_idx, dataset_name_to_label[file]))\n",
    "\n",
    "    start_idx += len(log_texts)\n",
    "    print(f\"Processed {file}: {len(log_texts)} logs\")\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(index, os.path.join(RAG_FOLDER, \"logs.index\"))\n",
    "pickle.dump(log_mapping, open(os.path.join(RAG_FOLDER, \"log_mapping.pkl\"), \"wb\"))\n",
    "\n",
    "# Save metadata \n",
    "metadata_df = pd.DataFrame(metadata, columns=[\"dataset\", \"faiss_index\", \"dataset_id\"])\n",
    "metadata_df.to_csv(os.path.join(RAG_FOLDER, \"metadata.csv\"), index=False)\n",
    "\n",
    "# Cluster logs using HDBSCAN \n",
    "all_embeddings = np.vstack(embeddings_all)\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=50, metric=\"euclidean\")\n",
    "cluster_labels = clusterer.fit_predict(all_embeddings)\n",
    "valid_mask = cluster_labels != -1\n",
    "valid_embeddings = all_embeddings[valid_mask]\n",
    "valid_clusters = cluster_labels[valid_mask]\n",
    "\n",
    "# Save cluster metadata \n",
    "metadata_df = metadata_df.iloc[:len(cluster_labels)].copy()\n",
    "metadata_df['cluster'] = cluster_labels\n",
    "metadata_df.to_csv(os.path.join(RAG_FOLDER, \"metadata_with_clusters.csv\"), index=False)\n",
    "\n",
    "# Compute and save centroids\n",
    "centroids = []\n",
    "for cluster_id in np.unique(valid_clusters):\n",
    "    centroid = valid_embeddings[valid_clusters == cluster_id].mean(axis=0)\n",
    "    centroids.append(centroid)\n",
    "np.save(os.path.join(RAG_FOLDER, \"cluster_centroids.npy\"), np.vstack(centroids))\n",
    "print(\"Cluster centroids computed and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5218c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (prism_env)",
   "language": "python",
   "name": "prism_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
